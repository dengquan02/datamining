{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence  \n",
    "from keras.models import Sequential  \n",
    "from keras.datasets import boston_housing  \n",
    "from keras.layers import Dense, Dropout \n",
    "# from keras.utils import multi_gpu_model  \n",
    "# from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from keras import regularizers  # 正则化  \n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "def distance(true_pt, pred_pt):\n",
    "    def rad(d):\n",
    "        return d * math.pi / 180.0\n",
    "    lat1 = true_pt[1]\n",
    "    lng1 = true_pt[0]\n",
    "    lat2 = pred_pt[1]\n",
    "    lng2 = pred_pt[0]\n",
    "    radLat1 = rad(lat1)\n",
    "    radLat2 = rad(lat2)\n",
    "    a = radLat1 - radLat2\n",
    "    b = rad(lng1) - rad(lng2)\n",
    "    print(a)\n",
    "    \n",
    "    s = 2 * tf.asin(tf.sqrt(tf.pow(tf.sin(a/2),2) +\n",
    "    tf.cos(radLat1)*tf.cos(radLat2)*tf.pow(tf.sin(b/2),2)))\n",
    "    s = s * 6378.137\n",
    "    s = round(s * 10000) / 10\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 度量两点距离\n",
    "import math\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0\n",
    "\n",
    "# 地理坐标系：为球面坐标。 参考平面地是椭球面，坐标单位：经纬度；\n",
    "# 投影坐标系：为平面坐标。参考平面地是水平面，坐标单位：米、千米等；\n",
    "# 地理坐标转换到投影坐标的过程可理解为投影。（投影：将不规则的地球曲面转换为平面）\n",
    "\n",
    "# 目前国内主要有三种地理坐标系\n",
    "# 1、WGS84坐标系：即地球坐标系（World Geodetic System），国际上通用的坐标系。\n",
    "# 设备包含的GPS芯片或者北斗芯片获取的经纬度一般都是为WGS84地理坐标系，目前谷歌地图采用的是WGS84坐标系（中国范围除外）。\n",
    "# 2、GCJ02坐标系：即火星坐标系，国测局坐标系。是由中国国家测绘局制定。由WGS84坐标系经加密后的坐标系。谷歌中国和搜搜中国采用。\n",
    "# 3、BD09坐标系：百度坐标系，GCJ02坐标系经加密后的坐标系。\n",
    "\n",
    "# 投影：墨卡托投影、高斯-克吕格 (Gauss-Krüger) 投影\n",
    "# 感兴趣的同学可以在https://desktop.arcgis.com/zh-cn/arcmap/10.3/guide-books/map-projections/list-of-supported-map-projections.htm深入了解\n",
    "\n",
    "# gps两点间距离（单位为米）\n",
    "def distance(true_pt, pred_pt):\n",
    "    lat1 = float(true_pt[1])\n",
    "    lng1 = float(true_pt[0])\n",
    "    lat2 = float(pred_pt[1])\n",
    "    lng2 = float(pred_pt[0])\n",
    "    radLat1 = rad(lat1)\n",
    "    radLat2 = rad(lat2)\n",
    "    a = radLat1 - radLat2\n",
    "    b = rad(lng1) - rad(lng2)\n",
    "    s = 2 * math.asin(math.sqrt(math.pow(math.sin(a/2),2) +\n",
    "    math.cos(radLat1)*math.cos(radLat2)*math.pow(math.sin(b/2),2)))\n",
    "    s = s * 6378.137\n",
    "    s = round(s * 10000) / 10\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle  \n",
    "\n",
    "# 加载数据集\n",
    "def load_data(data_path, shuff=True):\n",
    "    data = pd.read_csv(data_path)\n",
    "    data = data.fillna(-1)  # Nan值填充\n",
    "#     print(data.iloc[1000])\n",
    "    if shuff:   # 将数据集打乱\n",
    "        data = shuffle(data)\n",
    "#     print(data.iloc[1000])\n",
    "    # 构建数据集和标签\n",
    "    samples = []\n",
    "    labels = []\n",
    "    for index, row in data.iterrows():\n",
    "    # print(row) # 输出每行的索引值\n",
    "        matrix = []  # 7 * 4\n",
    "        for i in range(7):\n",
    "            matrix.append([i+1,\n",
    "                           row['AsuLevel_' + str(i+1)],\n",
    "                           row['SignalLevel_' + str(i+1)],\n",
    "                           row['Dbm_' + str(i+1)]]) # 新数据集中不存在RSSI，使用Dbm(代功率的绝对值)\n",
    "        # print(matrix)\n",
    "        samples.append(matrix)\n",
    "        labels.append([row['Longitude'], row['Latitude']])\n",
    "    return np.array(samples), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.  55.   4. -85.]\n",
      " [  2.  52.   4. -88.]\n",
      " [  3.  57.   4. -83.]\n",
      " [  4.  -1.  -1.  -1.]\n",
      " [  5.  -1.  -1.  -1.]\n",
      " [  6.  -1.  -1.  -1.]\n",
      " [  7.  -1.  -1.  -1.]] [121.4925751  31.2843533]\n",
      "(3479, 7, 4)\n",
      "(3479, 2)\n"
     ]
    }
   ],
   "source": [
    "path = r\"./siping_4g.csv\"\n",
    "samples, labels = load_data(path)\n",
    "print(samples[0], labels[0])\n",
    "print(samples.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. MinMaxScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#划分训练数据、训练标签、验证数据、验证标签\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 训练集归一化  \u001b[39;00m\n\u001b[0;32m      9\u001b[0m min_max_scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()  \n\u001b[1;32m---> 10\u001b[0m \u001b[43mmin_max_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     11\u001b[0m x_train \u001b[38;5;241m=\u001b[39m min_max_scaler\u001b[38;5;241m.\u001b[39mtransform(x_train)  \n\u001b[0;32m     13\u001b[0m min_max_scaler\u001b[38;5;241m.\u001b[39mfit(y_train_pd)  \n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:416\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:453\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMaxScaler does not support sparse input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using MaxAbsScaler instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    452\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 453\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m data_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    462\u001b[0m data_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:794\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    790\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to convert array of bytes/strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minto decimal numbers with dtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 794\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    797\u001b[0m     )\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    800\u001b[0m     _assert_all_finite(array, allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. MinMaxScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(samples, labels, test_size=0.2) \n",
    "\n",
    "#划分训练数据、训练标签、验证数据、验证标签\n",
    "\n",
    "# 训练集归一化  \n",
    "min_max_scaler = MinMaxScaler()  \n",
    "min_max_scaler.fit(x_train)  \n",
    "x_train = min_max_scaler.transform(x_train)  \n",
    "\n",
    "min_max_scaler.fit(y_train_pd)  \n",
    "y_train = min_max_scaler.transform(y_train)  \n",
    "  \n",
    "# 验证集归一化  \n",
    "min_max_scaler.fit(x_valid_pd)  \n",
    "x_valid = min_max_scaler.transform(x_valid)  \n",
    "\n",
    "min_max_scaler.fit(y_valid_pd)  \n",
    "y_valid = min_max_scaler.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3479, 7, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "# 数据变形： (3479, 7, 4)---》 (3479, 7, 4，1)\n",
    "samples = samples.reshape(-1, 7,4,1) # -1自动计算\n",
    "# samples = samples.reshape(samples.shape[0], -1)\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(2783, 7, 4, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [157]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m x_train, x_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m train_test_split(samples, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)  \u001b[38;5;66;03m#划分训练数据、训练标签、验证数据、验证标签\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 转成DataFrame格式方便数据处理\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m x_train_pd \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m      7\u001b[0m y_train_pd \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(y_train)  \n\u001b[0;32m      8\u001b[0m x_valid_pd \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(x_valid)  \n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tf\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tf\\lib\\site-packages\\pandas\\core\\internals\\construction.py:331\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    326\u001b[0m         values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_prep_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_on_sanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_equal(values\u001b[38;5;241m.\u001b[39mdtype, dtype):\n\u001b[0;32m    334\u001b[0m     shape \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tf\\lib\\site-packages\\pandas\\core\\internals\\construction.py:591\u001b[0m, in \u001b[0;36m_prep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    589\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(2783, 7, 4, 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 7, 4, 64)          640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 4, 2, 64)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 4, 2, 32)          18464     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 2, 1, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 2, 1, 10)          330       \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 2, 1, 10)          0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 2, 1, 15)          165       \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 2, 1, 36)          576       \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 2, 1, 15)          555       \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 2, 1, 2)           32        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,762\n",
      "Trainable params: 20,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_28\" is incompatible with the layer: expected shape=(None, 7, 4, 1), found shape=(None, 28)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [154]\u001b[0m, in \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# 损失均方误差\u001b[39;00m\n\u001b[0;32m     55\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# 优化器\u001b[39;00m\n\u001b[0;32m     56\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     57\u001b[0m               )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# model.compile(optimizer='adam', loss='mse', metrics=[distance])\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 迭代次数\u001b[39;49;00m\n\u001b[0;32m     61\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 每次用来梯度下降的批处理数据大小\u001b[39;49;00m\n\u001b[0;32m     62\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：输出训练进度，2：输出每一个epoch\u001b[39;49;00m\n\u001b[0;32m     63\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 验证集\u001b[39;49;00m\n\u001b[0;32m     64\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\__autograph_generated_filem_dyvk7t.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\CONDA\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_28\" is incompatible with the layer: expected shape=(None, 7, 4, 1), found shape=(None, 28)\n"
     ]
    }
   ],
   "source": [
    "# 单CPU or GPU版本，若有GPU则自动切换\n",
    "from keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten, Dropout\n",
    "model = Sequential()  # 初始化，很重要！\n",
    "model.add(Convolution2D(input_shape=(7, 4, 1),\n",
    "          filters=64,\n",
    "          kernel_size=3,\n",
    "          strides=1,\n",
    "          padding='same',\n",
    "          activation='relu'))\n",
    "\n",
    "# 第二个池化层  14*14\n",
    "model.add(MaxPooling2D(2, 2, 'same'))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "          filters=32,\n",
    "          kernel_size=3,\n",
    "          strides=1,\n",
    "          padding='same',\n",
    "          activation='relu'))\n",
    "\n",
    "# 第二个池化层  14*14\n",
    "model.add(MaxPooling2D(2, 2, 'same'))\n",
    "\n",
    "# 把第二个池化层的输出扁平化为一维   7*7\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(units=10,   # 输出大小\n",
    "                activation='relu',  # 激励函数\n",
    "                )\n",
    "          )\n",
    "\n",
    "model.add(Dropout(0.2))  # 丢弃神经元链接概率\n",
    "\n",
    "model.add(Dense(units=15,\n",
    "                #                 kernel_regularizer=regularizers.l2(0.01),  # 施加在权重上的正则项\n",
    "                #                 activity_regularizer=regularizers.l1(0.01),  # 施加在输出上的正则项\n",
    "                activation='relu',  # 激励函数\n",
    "                #                  bias_regularizer=regularizers.l1_l2(0.01)  # 施加在偏置向量上的正则项\n",
    "                )\n",
    "          )\n",
    "\n",
    "#######\n",
    "model.add(Dense(units=36, activation='relu'))\n",
    "model.add(Dense(units=15, activation='relu'))\n",
    "\n",
    "#######\n",
    "\n",
    "model.add(Dense(units=2,\n",
    "                activation='linear'  # 线性激励函数 回归一般在输出层用这个激励函数\n",
    "                )\n",
    "          )\n",
    "\n",
    "print(model.summary())  # 打印网络层次结构\n",
    "\n",
    "model.compile(loss='mae',  # 损失均方误差\n",
    "              optimizer='adam',  # 优化器\n",
    "              metrics=['mae']\n",
    "              )\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=[distance])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200,  # 迭代次数\n",
    "                    batch_size=200,  # 每次用来梯度下降的批处理数据大小\n",
    "                    verbose=2,  # verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：输出训练进度，2：输出每一个epoch\n",
    "                    validation_data=(x_valid, y_valid)  # 验证集\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_109 (Dense)           (None, 20)                580       \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 15)                315       \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 2)                 32        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 927\n",
      "Trainable params: 927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "14/14 - 1s - loss: 0.3886 - mae: 0.3886 - val_loss: 0.2641 - val_mae: 0.2641 - 576ms/epoch - 41ms/step\n",
      "Epoch 2/200\n",
      "14/14 - 0s - loss: 0.3275 - mae: 0.3275 - val_loss: 0.2532 - val_mae: 0.2532 - 32ms/epoch - 2ms/step\n",
      "Epoch 3/200\n",
      "14/14 - 0s - loss: 0.2893 - mae: 0.2893 - val_loss: 0.2457 - val_mae: 0.2457 - 33ms/epoch - 2ms/step\n",
      "Epoch 4/200\n",
      "14/14 - 0s - loss: 0.2706 - mae: 0.2706 - val_loss: 0.2371 - val_mae: 0.2371 - 31ms/epoch - 2ms/step\n",
      "Epoch 5/200\n",
      "14/14 - 0s - loss: 0.2592 - mae: 0.2592 - val_loss: 0.2320 - val_mae: 0.2320 - 31ms/epoch - 2ms/step\n",
      "Epoch 6/200\n",
      "14/14 - 0s - loss: 0.2482 - mae: 0.2482 - val_loss: 0.2270 - val_mae: 0.2270 - 30ms/epoch - 2ms/step\n",
      "Epoch 7/200\n",
      "14/14 - 0s - loss: 0.2418 - mae: 0.2418 - val_loss: 0.2267 - val_mae: 0.2267 - 31ms/epoch - 2ms/step\n",
      "Epoch 8/200\n",
      "14/14 - 0s - loss: 0.2417 - mae: 0.2417 - val_loss: 0.2248 - val_mae: 0.2248 - 32ms/epoch - 2ms/step\n",
      "Epoch 9/200\n",
      "14/14 - 0s - loss: 0.2368 - mae: 0.2368 - val_loss: 0.2236 - val_mae: 0.2236 - 33ms/epoch - 2ms/step\n",
      "Epoch 10/200\n",
      "14/14 - 0s - loss: 0.2339 - mae: 0.2339 - val_loss: 0.2238 - val_mae: 0.2238 - 32ms/epoch - 2ms/step\n",
      "Epoch 11/200\n",
      "14/14 - 0s - loss: 0.2310 - mae: 0.2310 - val_loss: 0.2211 - val_mae: 0.2211 - 31ms/epoch - 2ms/step\n",
      "Epoch 12/200\n",
      "14/14 - 0s - loss: 0.2310 - mae: 0.2310 - val_loss: 0.2209 - val_mae: 0.2209 - 31ms/epoch - 2ms/step\n",
      "Epoch 13/200\n",
      "14/14 - 0s - loss: 0.2301 - mae: 0.2301 - val_loss: 0.2196 - val_mae: 0.2196 - 31ms/epoch - 2ms/step\n",
      "Epoch 14/200\n",
      "14/14 - 0s - loss: 0.2299 - mae: 0.2299 - val_loss: 0.2197 - val_mae: 0.2197 - 35ms/epoch - 2ms/step\n",
      "Epoch 15/200\n",
      "14/14 - 0s - loss: 0.2282 - mae: 0.2282 - val_loss: 0.2182 - val_mae: 0.2182 - 31ms/epoch - 2ms/step\n",
      "Epoch 16/200\n",
      "14/14 - 0s - loss: 0.2288 - mae: 0.2288 - val_loss: 0.2180 - val_mae: 0.2180 - 32ms/epoch - 2ms/step\n",
      "Epoch 17/200\n",
      "14/14 - 0s - loss: 0.2247 - mae: 0.2247 - val_loss: 0.2173 - val_mae: 0.2173 - 31ms/epoch - 2ms/step\n",
      "Epoch 18/200\n",
      "14/14 - 0s - loss: 0.2263 - mae: 0.2263 - val_loss: 0.2163 - val_mae: 0.2163 - 31ms/epoch - 2ms/step\n",
      "Epoch 19/200\n",
      "14/14 - 0s - loss: 0.2246 - mae: 0.2246 - val_loss: 0.2162 - val_mae: 0.2162 - 30ms/epoch - 2ms/step\n",
      "Epoch 20/200\n",
      "14/14 - 0s - loss: 0.2245 - mae: 0.2245 - val_loss: 0.2154 - val_mae: 0.2154 - 33ms/epoch - 2ms/step\n",
      "Epoch 21/200\n",
      "14/14 - 0s - loss: 0.2238 - mae: 0.2238 - val_loss: 0.2155 - val_mae: 0.2155 - 32ms/epoch - 2ms/step\n",
      "Epoch 22/200\n",
      "14/14 - 0s - loss: 0.2234 - mae: 0.2234 - val_loss: 0.2141 - val_mae: 0.2141 - 31ms/epoch - 2ms/step\n",
      "Epoch 23/200\n",
      "14/14 - 0s - loss: 0.2234 - mae: 0.2234 - val_loss: 0.2132 - val_mae: 0.2132 - 31ms/epoch - 2ms/step\n",
      "Epoch 24/200\n",
      "14/14 - 0s - loss: 0.2214 - mae: 0.2214 - val_loss: 0.2125 - val_mae: 0.2125 - 32ms/epoch - 2ms/step\n",
      "Epoch 25/200\n",
      "14/14 - 0s - loss: 0.2202 - mae: 0.2202 - val_loss: 0.2122 - val_mae: 0.2122 - 30ms/epoch - 2ms/step\n",
      "Epoch 26/200\n",
      "14/14 - 0s - loss: 0.2220 - mae: 0.2220 - val_loss: 0.2114 - val_mae: 0.2114 - 32ms/epoch - 2ms/step\n",
      "Epoch 27/200\n",
      "14/14 - 0s - loss: 0.2207 - mae: 0.2207 - val_loss: 0.2111 - val_mae: 0.2111 - 30ms/epoch - 2ms/step\n",
      "Epoch 28/200\n",
      "14/14 - 0s - loss: 0.2202 - mae: 0.2202 - val_loss: 0.2111 - val_mae: 0.2111 - 30ms/epoch - 2ms/step\n",
      "Epoch 29/200\n",
      "14/14 - 0s - loss: 0.2202 - mae: 0.2202 - val_loss: 0.2108 - val_mae: 0.2108 - 30ms/epoch - 2ms/step\n",
      "Epoch 30/200\n",
      "14/14 - 0s - loss: 0.2200 - mae: 0.2200 - val_loss: 0.2106 - val_mae: 0.2106 - 32ms/epoch - 2ms/step\n",
      "Epoch 31/200\n",
      "14/14 - 0s - loss: 0.2200 - mae: 0.2200 - val_loss: 0.2110 - val_mae: 0.2110 - 33ms/epoch - 2ms/step\n",
      "Epoch 32/200\n",
      "14/14 - 0s - loss: 0.2186 - mae: 0.2186 - val_loss: 0.2103 - val_mae: 0.2103 - 30ms/epoch - 2ms/step\n",
      "Epoch 33/200\n",
      "14/14 - 0s - loss: 0.2175 - mae: 0.2175 - val_loss: 0.2112 - val_mae: 0.2112 - 31ms/epoch - 2ms/step\n",
      "Epoch 34/200\n",
      "14/14 - 0s - loss: 0.2187 - mae: 0.2187 - val_loss: 0.2098 - val_mae: 0.2098 - 32ms/epoch - 2ms/step\n",
      "Epoch 35/200\n",
      "14/14 - 0s - loss: 0.2170 - mae: 0.2170 - val_loss: 0.2093 - val_mae: 0.2093 - 31ms/epoch - 2ms/step\n",
      "Epoch 36/200\n",
      "14/14 - 0s - loss: 0.2166 - mae: 0.2166 - val_loss: 0.2090 - val_mae: 0.2090 - 30ms/epoch - 2ms/step\n",
      "Epoch 37/200\n",
      "14/14 - 0s - loss: 0.2170 - mae: 0.2170 - val_loss: 0.2100 - val_mae: 0.2100 - 32ms/epoch - 2ms/step\n",
      "Epoch 38/200\n",
      "14/14 - 0s - loss: 0.2159 - mae: 0.2159 - val_loss: 0.2096 - val_mae: 0.2096 - 32ms/epoch - 2ms/step\n",
      "Epoch 39/200\n",
      "14/14 - 0s - loss: 0.2150 - mae: 0.2150 - val_loss: 0.2090 - val_mae: 0.2090 - 30ms/epoch - 2ms/step\n",
      "Epoch 40/200\n",
      "14/14 - 0s - loss: 0.2162 - mae: 0.2162 - val_loss: 0.2083 - val_mae: 0.2083 - 32ms/epoch - 2ms/step\n",
      "Epoch 41/200\n",
      "14/14 - 0s - loss: 0.2156 - mae: 0.2156 - val_loss: 0.2083 - val_mae: 0.2083 - 32ms/epoch - 2ms/step\n",
      "Epoch 42/200\n",
      "14/14 - 0s - loss: 0.2151 - mae: 0.2151 - val_loss: 0.2096 - val_mae: 0.2096 - 31ms/epoch - 2ms/step\n",
      "Epoch 43/200\n",
      "14/14 - 0s - loss: 0.2155 - mae: 0.2155 - val_loss: 0.2075 - val_mae: 0.2075 - 32ms/epoch - 2ms/step\n",
      "Epoch 44/200\n",
      "14/14 - 0s - loss: 0.2138 - mae: 0.2138 - val_loss: 0.2070 - val_mae: 0.2070 - 32ms/epoch - 2ms/step\n",
      "Epoch 45/200\n",
      "14/14 - 0s - loss: 0.2139 - mae: 0.2139 - val_loss: 0.2065 - val_mae: 0.2065 - 32ms/epoch - 2ms/step\n",
      "Epoch 46/200\n",
      "14/14 - 0s - loss: 0.2143 - mae: 0.2143 - val_loss: 0.2086 - val_mae: 0.2086 - 31ms/epoch - 2ms/step\n",
      "Epoch 47/200\n",
      "14/14 - 0s - loss: 0.2138 - mae: 0.2138 - val_loss: 0.2081 - val_mae: 0.2081 - 32ms/epoch - 2ms/step\n",
      "Epoch 48/200\n",
      "14/14 - 0s - loss: 0.2127 - mae: 0.2127 - val_loss: 0.2078 - val_mae: 0.2078 - 31ms/epoch - 2ms/step\n",
      "Epoch 49/200\n",
      "14/14 - 0s - loss: 0.2121 - mae: 0.2121 - val_loss: 0.2076 - val_mae: 0.2076 - 31ms/epoch - 2ms/step\n",
      "Epoch 50/200\n",
      "14/14 - 0s - loss: 0.2134 - mae: 0.2134 - val_loss: 0.2068 - val_mae: 0.2068 - 32ms/epoch - 2ms/step\n",
      "Epoch 51/200\n",
      "14/14 - 0s - loss: 0.2118 - mae: 0.2118 - val_loss: 0.2091 - val_mae: 0.2091 - 31ms/epoch - 2ms/step\n",
      "Epoch 52/200\n",
      "14/14 - 0s - loss: 0.2113 - mae: 0.2113 - val_loss: 0.2081 - val_mae: 0.2081 - 31ms/epoch - 2ms/step\n",
      "Epoch 53/200\n",
      "14/14 - 0s - loss: 0.2114 - mae: 0.2114 - val_loss: 0.2072 - val_mae: 0.2072 - 30ms/epoch - 2ms/step\n",
      "Epoch 54/200\n",
      "14/14 - 0s - loss: 0.2105 - mae: 0.2105 - val_loss: 0.2090 - val_mae: 0.2090 - 29ms/epoch - 2ms/step\n",
      "Epoch 55/200\n",
      "14/14 - 0s - loss: 0.2117 - mae: 0.2117 - val_loss: 0.2078 - val_mae: 0.2078 - 40ms/epoch - 3ms/step\n",
      "Epoch 56/200\n",
      "14/14 - 0s - loss: 0.2107 - mae: 0.2107 - val_loss: 0.2073 - val_mae: 0.2073 - 30ms/epoch - 2ms/step\n",
      "Epoch 57/200\n",
      "14/14 - 0s - loss: 0.2114 - mae: 0.2114 - val_loss: 0.2093 - val_mae: 0.2093 - 31ms/epoch - 2ms/step\n",
      "Epoch 58/200\n",
      "14/14 - 0s - loss: 0.2108 - mae: 0.2108 - val_loss: 0.2076 - val_mae: 0.2076 - 31ms/epoch - 2ms/step\n",
      "Epoch 59/200\n",
      "14/14 - 0s - loss: 0.2101 - mae: 0.2101 - val_loss: 0.2080 - val_mae: 0.2080 - 31ms/epoch - 2ms/step\n",
      "Epoch 60/200\n",
      "14/14 - 0s - loss: 0.2096 - mae: 0.2096 - val_loss: 0.2079 - val_mae: 0.2079 - 29ms/epoch - 2ms/step\n",
      "Epoch 61/200\n",
      "14/14 - 0s - loss: 0.2100 - mae: 0.2100 - val_loss: 0.2078 - val_mae: 0.2078 - 35ms/epoch - 2ms/step\n",
      "Epoch 62/200\n",
      "14/14 - 0s - loss: 0.2091 - mae: 0.2091 - val_loss: 0.2097 - val_mae: 0.2097 - 30ms/epoch - 2ms/step\n",
      "Epoch 63/200\n",
      "14/14 - 0s - loss: 0.2097 - mae: 0.2097 - val_loss: 0.2076 - val_mae: 0.2076 - 32ms/epoch - 2ms/step\n",
      "Epoch 64/200\n",
      "14/14 - 0s - loss: 0.2087 - mae: 0.2087 - val_loss: 0.2073 - val_mae: 0.2073 - 31ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200\n",
      "14/14 - 0s - loss: 0.2086 - mae: 0.2086 - val_loss: 0.2068 - val_mae: 0.2068 - 32ms/epoch - 2ms/step\n",
      "Epoch 66/200\n",
      "14/14 - 0s - loss: 0.2090 - mae: 0.2090 - val_loss: 0.2064 - val_mae: 0.2064 - 30ms/epoch - 2ms/step\n",
      "Epoch 67/200\n",
      "14/14 - 0s - loss: 0.2078 - mae: 0.2078 - val_loss: 0.2081 - val_mae: 0.2081 - 31ms/epoch - 2ms/step\n",
      "Epoch 68/200\n",
      "14/14 - 0s - loss: 0.2096 - mae: 0.2096 - val_loss: 0.2070 - val_mae: 0.2070 - 31ms/epoch - 2ms/step\n",
      "Epoch 69/200\n",
      "14/14 - 0s - loss: 0.2093 - mae: 0.2093 - val_loss: 0.2081 - val_mae: 0.2081 - 32ms/epoch - 2ms/step\n",
      "Epoch 70/200\n",
      "14/14 - 0s - loss: 0.2078 - mae: 0.2078 - val_loss: 0.2073 - val_mae: 0.2073 - 31ms/epoch - 2ms/step\n",
      "Epoch 71/200\n",
      "14/14 - 0s - loss: 0.2082 - mae: 0.2082 - val_loss: 0.2056 - val_mae: 0.2056 - 30ms/epoch - 2ms/step\n",
      "Epoch 72/200\n",
      "14/14 - 0s - loss: 0.2074 - mae: 0.2074 - val_loss: 0.2088 - val_mae: 0.2088 - 29ms/epoch - 2ms/step\n",
      "Epoch 73/200\n",
      "14/14 - 0s - loss: 0.2085 - mae: 0.2085 - val_loss: 0.2074 - val_mae: 0.2074 - 31ms/epoch - 2ms/step\n",
      "Epoch 74/200\n",
      "14/14 - 0s - loss: 0.2086 - mae: 0.2086 - val_loss: 0.2072 - val_mae: 0.2072 - 32ms/epoch - 2ms/step\n",
      "Epoch 75/200\n",
      "14/14 - 0s - loss: 0.2073 - mae: 0.2073 - val_loss: 0.2080 - val_mae: 0.2080 - 30ms/epoch - 2ms/step\n",
      "Epoch 76/200\n",
      "14/14 - 0s - loss: 0.2082 - mae: 0.2082 - val_loss: 0.2073 - val_mae: 0.2073 - 31ms/epoch - 2ms/step\n",
      "Epoch 77/200\n",
      "14/14 - 0s - loss: 0.2074 - mae: 0.2074 - val_loss: 0.2066 - val_mae: 0.2066 - 32ms/epoch - 2ms/step\n",
      "Epoch 78/200\n",
      "14/14 - 0s - loss: 0.2066 - mae: 0.2066 - val_loss: 0.2119 - val_mae: 0.2119 - 30ms/epoch - 2ms/step\n",
      "Epoch 79/200\n",
      "14/14 - 0s - loss: 0.2070 - mae: 0.2070 - val_loss: 0.2049 - val_mae: 0.2049 - 31ms/epoch - 2ms/step\n",
      "Epoch 80/200\n",
      "14/14 - 0s - loss: 0.2074 - mae: 0.2074 - val_loss: 0.2111 - val_mae: 0.2111 - 31ms/epoch - 2ms/step\n",
      "Epoch 81/200\n",
      "14/14 - 0s - loss: 0.2065 - mae: 0.2065 - val_loss: 0.2077 - val_mae: 0.2077 - 32ms/epoch - 2ms/step\n",
      "Epoch 82/200\n",
      "14/14 - 0s - loss: 0.2072 - mae: 0.2072 - val_loss: 0.2094 - val_mae: 0.2094 - 31ms/epoch - 2ms/step\n",
      "Epoch 83/200\n",
      "14/14 - 0s - loss: 0.2073 - mae: 0.2073 - val_loss: 0.2076 - val_mae: 0.2076 - 32ms/epoch - 2ms/step\n",
      "Epoch 84/200\n",
      "14/14 - 0s - loss: 0.2075 - mae: 0.2075 - val_loss: 0.2077 - val_mae: 0.2077 - 30ms/epoch - 2ms/step\n",
      "Epoch 85/200\n",
      "14/14 - 0s - loss: 0.2079 - mae: 0.2079 - val_loss: 0.2066 - val_mae: 0.2066 - 30ms/epoch - 2ms/step\n",
      "Epoch 86/200\n",
      "14/14 - 0s - loss: 0.2079 - mae: 0.2079 - val_loss: 0.2076 - val_mae: 0.2076 - 30ms/epoch - 2ms/step\n",
      "Epoch 87/200\n",
      "14/14 - 0s - loss: 0.2067 - mae: 0.2067 - val_loss: 0.2075 - val_mae: 0.2075 - 32ms/epoch - 2ms/step\n",
      "Epoch 88/200\n",
      "14/14 - 0s - loss: 0.2066 - mae: 0.2066 - val_loss: 0.2079 - val_mae: 0.2079 - 31ms/epoch - 2ms/step\n",
      "Epoch 89/200\n",
      "14/14 - 0s - loss: 0.2066 - mae: 0.2066 - val_loss: 0.2084 - val_mae: 0.2084 - 30ms/epoch - 2ms/step\n",
      "Epoch 90/200\n",
      "14/14 - 0s - loss: 0.2078 - mae: 0.2078 - val_loss: 0.2062 - val_mae: 0.2062 - 31ms/epoch - 2ms/step\n",
      "Epoch 91/200\n",
      "14/14 - 0s - loss: 0.2062 - mae: 0.2062 - val_loss: 0.2078 - val_mae: 0.2078 - 31ms/epoch - 2ms/step\n",
      "Epoch 92/200\n",
      "14/14 - 0s - loss: 0.2057 - mae: 0.2057 - val_loss: 0.2079 - val_mae: 0.2079 - 33ms/epoch - 2ms/step\n",
      "Epoch 93/200\n",
      "14/14 - 0s - loss: 0.2055 - mae: 0.2055 - val_loss: 0.2065 - val_mae: 0.2065 - 32ms/epoch - 2ms/step\n",
      "Epoch 94/200\n",
      "14/14 - 0s - loss: 0.2062 - mae: 0.2062 - val_loss: 0.2089 - val_mae: 0.2089 - 31ms/epoch - 2ms/step\n",
      "Epoch 95/200\n",
      "14/14 - 0s - loss: 0.2068 - mae: 0.2068 - val_loss: 0.2051 - val_mae: 0.2051 - 30ms/epoch - 2ms/step\n",
      "Epoch 96/200\n",
      "14/14 - 0s - loss: 0.2055 - mae: 0.2055 - val_loss: 0.2079 - val_mae: 0.2079 - 31ms/epoch - 2ms/step\n",
      "Epoch 97/200\n",
      "14/14 - 0s - loss: 0.2036 - mae: 0.2036 - val_loss: 0.2080 - val_mae: 0.2080 - 31ms/epoch - 2ms/step\n",
      "Epoch 98/200\n",
      "14/14 - 0s - loss: 0.2080 - mae: 0.2080 - val_loss: 0.2074 - val_mae: 0.2074 - 33ms/epoch - 2ms/step\n",
      "Epoch 99/200\n",
      "14/14 - 0s - loss: 0.2062 - mae: 0.2062 - val_loss: 0.2056 - val_mae: 0.2056 - 31ms/epoch - 2ms/step\n",
      "Epoch 100/200\n",
      "14/14 - 0s - loss: 0.2050 - mae: 0.2050 - val_loss: 0.2056 - val_mae: 0.2056 - 30ms/epoch - 2ms/step\n",
      "Epoch 101/200\n",
      "14/14 - 0s - loss: 0.2059 - mae: 0.2059 - val_loss: 0.2107 - val_mae: 0.2107 - 31ms/epoch - 2ms/step\n",
      "Epoch 102/200\n",
      "14/14 - 0s - loss: 0.2049 - mae: 0.2049 - val_loss: 0.2069 - val_mae: 0.2069 - 31ms/epoch - 2ms/step\n",
      "Epoch 103/200\n",
      "14/14 - 0s - loss: 0.2053 - mae: 0.2053 - val_loss: 0.2065 - val_mae: 0.2065 - 32ms/epoch - 2ms/step\n",
      "Epoch 104/200\n",
      "14/14 - 0s - loss: 0.2062 - mae: 0.2062 - val_loss: 0.2103 - val_mae: 0.2103 - 31ms/epoch - 2ms/step\n",
      "Epoch 105/200\n",
      "14/14 - 0s - loss: 0.2056 - mae: 0.2056 - val_loss: 0.2059 - val_mae: 0.2059 - 32ms/epoch - 2ms/step\n",
      "Epoch 106/200\n",
      "14/14 - 0s - loss: 0.2054 - mae: 0.2054 - val_loss: 0.2072 - val_mae: 0.2072 - 30ms/epoch - 2ms/step\n",
      "Epoch 107/200\n",
      "14/14 - 0s - loss: 0.2054 - mae: 0.2054 - val_loss: 0.2100 - val_mae: 0.2100 - 30ms/epoch - 2ms/step\n",
      "Epoch 108/200\n",
      "14/14 - 0s - loss: 0.2049 - mae: 0.2049 - val_loss: 0.2041 - val_mae: 0.2041 - 31ms/epoch - 2ms/step\n",
      "Epoch 109/200\n",
      "14/14 - 0s - loss: 0.2049 - mae: 0.2049 - val_loss: 0.2079 - val_mae: 0.2079 - 32ms/epoch - 2ms/step\n",
      "Epoch 110/200\n",
      "14/14 - 0s - loss: 0.2054 - mae: 0.2054 - val_loss: 0.2070 - val_mae: 0.2070 - 31ms/epoch - 2ms/step\n",
      "Epoch 111/200\n",
      "14/14 - 0s - loss: 0.2047 - mae: 0.2047 - val_loss: 0.2084 - val_mae: 0.2084 - 31ms/epoch - 2ms/step\n",
      "Epoch 112/200\n",
      "14/14 - 0s - loss: 0.2038 - mae: 0.2038 - val_loss: 0.2090 - val_mae: 0.2090 - 31ms/epoch - 2ms/step\n",
      "Epoch 113/200\n",
      "14/14 - 0s - loss: 0.2042 - mae: 0.2042 - val_loss: 0.2085 - val_mae: 0.2085 - 31ms/epoch - 2ms/step\n",
      "Epoch 114/200\n",
      "14/14 - 0s - loss: 0.2060 - mae: 0.2060 - val_loss: 0.2032 - val_mae: 0.2032 - 31ms/epoch - 2ms/step\n",
      "Epoch 115/200\n",
      "14/14 - 0s - loss: 0.2045 - mae: 0.2045 - val_loss: 0.2075 - val_mae: 0.2075 - 30ms/epoch - 2ms/step\n",
      "Epoch 116/200\n",
      "14/14 - 0s - loss: 0.2038 - mae: 0.2038 - val_loss: 0.2064 - val_mae: 0.2064 - 30ms/epoch - 2ms/step\n",
      "Epoch 117/200\n",
      "14/14 - 0s - loss: 0.2043 - mae: 0.2043 - val_loss: 0.2050 - val_mae: 0.2050 - 31ms/epoch - 2ms/step\n",
      "Epoch 118/200\n",
      "14/14 - 0s - loss: 0.2049 - mae: 0.2049 - val_loss: 0.2095 - val_mae: 0.2095 - 31ms/epoch - 2ms/step\n",
      "Epoch 119/200\n",
      "14/14 - 0s - loss: 0.2032 - mae: 0.2032 - val_loss: 0.2046 - val_mae: 0.2046 - 31ms/epoch - 2ms/step\n",
      "Epoch 120/200\n",
      "14/14 - 0s - loss: 0.2052 - mae: 0.2052 - val_loss: 0.2093 - val_mae: 0.2093 - 30ms/epoch - 2ms/step\n",
      "Epoch 121/200\n",
      "14/14 - 0s - loss: 0.2037 - mae: 0.2037 - val_loss: 0.2066 - val_mae: 0.2066 - 30ms/epoch - 2ms/step\n",
      "Epoch 122/200\n",
      "14/14 - 0s - loss: 0.2038 - mae: 0.2038 - val_loss: 0.2055 - val_mae: 0.2055 - 30ms/epoch - 2ms/step\n",
      "Epoch 123/200\n",
      "14/14 - 0s - loss: 0.2047 - mae: 0.2047 - val_loss: 0.2055 - val_mae: 0.2055 - 35ms/epoch - 3ms/step\n",
      "Epoch 124/200\n",
      "14/14 - 0s - loss: 0.2035 - mae: 0.2035 - val_loss: 0.2077 - val_mae: 0.2077 - 31ms/epoch - 2ms/step\n",
      "Epoch 125/200\n",
      "14/14 - 0s - loss: 0.2034 - mae: 0.2034 - val_loss: 0.2066 - val_mae: 0.2066 - 31ms/epoch - 2ms/step\n",
      "Epoch 126/200\n",
      "14/14 - 0s - loss: 0.2040 - mae: 0.2040 - val_loss: 0.2055 - val_mae: 0.2055 - 31ms/epoch - 2ms/step\n",
      "Epoch 127/200\n",
      "14/14 - 0s - loss: 0.2030 - mae: 0.2030 - val_loss: 0.2064 - val_mae: 0.2064 - 30ms/epoch - 2ms/step\n",
      "Epoch 128/200\n",
      "14/14 - 0s - loss: 0.2029 - mae: 0.2029 - val_loss: 0.2095 - val_mae: 0.2095 - 30ms/epoch - 2ms/step\n",
      "Epoch 129/200\n",
      "14/14 - 0s - loss: 0.2039 - mae: 0.2039 - val_loss: 0.2035 - val_mae: 0.2035 - 31ms/epoch - 2ms/step\n",
      "Epoch 130/200\n",
      "14/14 - 0s - loss: 0.2028 - mae: 0.2028 - val_loss: 0.2063 - val_mae: 0.2063 - 31ms/epoch - 2ms/step\n",
      "Epoch 131/200\n",
      "14/14 - 0s - loss: 0.2037 - mae: 0.2037 - val_loss: 0.2059 - val_mae: 0.2059 - 38ms/epoch - 3ms/step\n",
      "Epoch 132/200\n",
      "14/14 - 0s - loss: 0.2035 - mae: 0.2035 - val_loss: 0.2044 - val_mae: 0.2044 - 30ms/epoch - 2ms/step\n",
      "Epoch 133/200\n",
      "14/14 - 0s - loss: 0.2040 - mae: 0.2040 - val_loss: 0.2079 - val_mae: 0.2079 - 30ms/epoch - 2ms/step\n",
      "Epoch 134/200\n",
      "14/14 - 0s - loss: 0.2035 - mae: 0.2035 - val_loss: 0.2064 - val_mae: 0.2064 - 31ms/epoch - 2ms/step\n",
      "Epoch 135/200\n",
      "14/14 - 0s - loss: 0.2029 - mae: 0.2029 - val_loss: 0.2039 - val_mae: 0.2039 - 31ms/epoch - 2ms/step\n",
      "Epoch 136/200\n",
      "14/14 - 0s - loss: 0.2043 - mae: 0.2043 - val_loss: 0.2071 - val_mae: 0.2071 - 32ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/200\n",
      "14/14 - 0s - loss: 0.2030 - mae: 0.2030 - val_loss: 0.2067 - val_mae: 0.2067 - 29ms/epoch - 2ms/step\n",
      "Epoch 138/200\n",
      "14/14 - 0s - loss: 0.2030 - mae: 0.2030 - val_loss: 0.2066 - val_mae: 0.2066 - 32ms/epoch - 2ms/step\n",
      "Epoch 139/200\n",
      "14/14 - 0s - loss: 0.2024 - mae: 0.2024 - val_loss: 0.2055 - val_mae: 0.2055 - 32ms/epoch - 2ms/step\n",
      "Epoch 140/200\n",
      "14/14 - 0s - loss: 0.2015 - mae: 0.2015 - val_loss: 0.2037 - val_mae: 0.2037 - 30ms/epoch - 2ms/step\n",
      "Epoch 141/200\n",
      "14/14 - 0s - loss: 0.2028 - mae: 0.2028 - val_loss: 0.2059 - val_mae: 0.2059 - 33ms/epoch - 2ms/step\n",
      "Epoch 142/200\n",
      "14/14 - 0s - loss: 0.2027 - mae: 0.2027 - val_loss: 0.2031 - val_mae: 0.2031 - 30ms/epoch - 2ms/step\n",
      "Epoch 143/200\n",
      "14/14 - 0s - loss: 0.2024 - mae: 0.2024 - val_loss: 0.2040 - val_mae: 0.2040 - 30ms/epoch - 2ms/step\n",
      "Epoch 144/200\n",
      "14/14 - 0s - loss: 0.2030 - mae: 0.2030 - val_loss: 0.2047 - val_mae: 0.2047 - 29ms/epoch - 2ms/step\n",
      "Epoch 145/200\n",
      "14/14 - 0s - loss: 0.2029 - mae: 0.2029 - val_loss: 0.2054 - val_mae: 0.2054 - 30ms/epoch - 2ms/step\n",
      "Epoch 146/200\n",
      "14/14 - 0s - loss: 0.2025 - mae: 0.2025 - val_loss: 0.2059 - val_mae: 0.2059 - 31ms/epoch - 2ms/step\n",
      "Epoch 147/200\n",
      "14/14 - 0s - loss: 0.2015 - mae: 0.2015 - val_loss: 0.2049 - val_mae: 0.2049 - 31ms/epoch - 2ms/step\n",
      "Epoch 148/200\n",
      "14/14 - 0s - loss: 0.2014 - mae: 0.2014 - val_loss: 0.2023 - val_mae: 0.2023 - 30ms/epoch - 2ms/step\n",
      "Epoch 149/200\n",
      "14/14 - 0s - loss: 0.2019 - mae: 0.2019 - val_loss: 0.2047 - val_mae: 0.2047 - 31ms/epoch - 2ms/step\n",
      "Epoch 150/200\n",
      "14/14 - 0s - loss: 0.2026 - mae: 0.2026 - val_loss: 0.2050 - val_mae: 0.2050 - 30ms/epoch - 2ms/step\n",
      "Epoch 151/200\n",
      "14/14 - 0s - loss: 0.2030 - mae: 0.2030 - val_loss: 0.2024 - val_mae: 0.2024 - 31ms/epoch - 2ms/step\n",
      "Epoch 152/200\n",
      "14/14 - 0s - loss: 0.2013 - mae: 0.2013 - val_loss: 0.2054 - val_mae: 0.2054 - 30ms/epoch - 2ms/step\n",
      "Epoch 153/200\n",
      "14/14 - 0s - loss: 0.2015 - mae: 0.2015 - val_loss: 0.2035 - val_mae: 0.2035 - 30ms/epoch - 2ms/step\n",
      "Epoch 154/200\n",
      "14/14 - 0s - loss: 0.2027 - mae: 0.2027 - val_loss: 0.2057 - val_mae: 0.2057 - 32ms/epoch - 2ms/step\n",
      "Epoch 155/200\n",
      "14/14 - 0s - loss: 0.2014 - mae: 0.2014 - val_loss: 0.2051 - val_mae: 0.2051 - 34ms/epoch - 2ms/step\n",
      "Epoch 156/200\n",
      "14/14 - 0s - loss: 0.2021 - mae: 0.2021 - val_loss: 0.2034 - val_mae: 0.2034 - 30ms/epoch - 2ms/step\n",
      "Epoch 157/200\n",
      "14/14 - 0s - loss: 0.2019 - mae: 0.2019 - val_loss: 0.2024 - val_mae: 0.2024 - 29ms/epoch - 2ms/step\n",
      "Epoch 158/200\n",
      "14/14 - 0s - loss: 0.2019 - mae: 0.2019 - val_loss: 0.2036 - val_mae: 0.2036 - 31ms/epoch - 2ms/step\n",
      "Epoch 159/200\n",
      "14/14 - 0s - loss: 0.2015 - mae: 0.2015 - val_loss: 0.2030 - val_mae: 0.2030 - 31ms/epoch - 2ms/step\n",
      "Epoch 160/200\n",
      "14/14 - 0s - loss: 0.2020 - mae: 0.2020 - val_loss: 0.2069 - val_mae: 0.2069 - 31ms/epoch - 2ms/step\n",
      "Epoch 161/200\n",
      "14/14 - 0s - loss: 0.2013 - mae: 0.2013 - val_loss: 0.2050 - val_mae: 0.2050 - 31ms/epoch - 2ms/step\n",
      "Epoch 162/200\n",
      "14/14 - 0s - loss: 0.2009 - mae: 0.2009 - val_loss: 0.2053 - val_mae: 0.2053 - 31ms/epoch - 2ms/step\n",
      "Epoch 163/200\n",
      "14/14 - 0s - loss: 0.2028 - mae: 0.2028 - val_loss: 0.2054 - val_mae: 0.2054 - 31ms/epoch - 2ms/step\n",
      "Epoch 164/200\n",
      "14/14 - 0s - loss: 0.2012 - mae: 0.2012 - val_loss: 0.2033 - val_mae: 0.2033 - 31ms/epoch - 2ms/step\n",
      "Epoch 165/200\n",
      "14/14 - 0s - loss: 0.2009 - mae: 0.2009 - val_loss: 0.2059 - val_mae: 0.2059 - 30ms/epoch - 2ms/step\n",
      "Epoch 166/200\n",
      "14/14 - 0s - loss: 0.2010 - mae: 0.2010 - val_loss: 0.2031 - val_mae: 0.2031 - 30ms/epoch - 2ms/step\n",
      "Epoch 167/200\n",
      "14/14 - 0s - loss: 0.2013 - mae: 0.2013 - val_loss: 0.2087 - val_mae: 0.2087 - 29ms/epoch - 2ms/step\n",
      "Epoch 168/200\n",
      "14/14 - 0s - loss: 0.2020 - mae: 0.2020 - val_loss: 0.2018 - val_mae: 0.2018 - 31ms/epoch - 2ms/step\n",
      "Epoch 169/200\n",
      "14/14 - 0s - loss: 0.2024 - mae: 0.2024 - val_loss: 0.2036 - val_mae: 0.2036 - 30ms/epoch - 2ms/step\n",
      "Epoch 170/200\n",
      "14/14 - 0s - loss: 0.2000 - mae: 0.2000 - val_loss: 0.2036 - val_mae: 0.2036 - 31ms/epoch - 2ms/step\n",
      "Epoch 171/200\n",
      "14/14 - 0s - loss: 0.2010 - mae: 0.2010 - val_loss: 0.2068 - val_mae: 0.2068 - 31ms/epoch - 2ms/step\n",
      "Epoch 172/200\n",
      "14/14 - 0s - loss: 0.2001 - mae: 0.2001 - val_loss: 0.2022 - val_mae: 0.2022 - 30ms/epoch - 2ms/step\n",
      "Epoch 173/200\n",
      "14/14 - 0s - loss: 0.2007 - mae: 0.2007 - val_loss: 0.2042 - val_mae: 0.2042 - 32ms/epoch - 2ms/step\n",
      "Epoch 174/200\n",
      "14/14 - 0s - loss: 0.2001 - mae: 0.2001 - val_loss: 0.2030 - val_mae: 0.2030 - 30ms/epoch - 2ms/step\n",
      "Epoch 175/200\n",
      "14/14 - 0s - loss: 0.2004 - mae: 0.2004 - val_loss: 0.2036 - val_mae: 0.2036 - 32ms/epoch - 2ms/step\n",
      "Epoch 176/200\n",
      "14/14 - 0s - loss: 0.2010 - mae: 0.2010 - val_loss: 0.2035 - val_mae: 0.2035 - 31ms/epoch - 2ms/step\n",
      "Epoch 177/200\n",
      "14/14 - 0s - loss: 0.2008 - mae: 0.2008 - val_loss: 0.2053 - val_mae: 0.2053 - 30ms/epoch - 2ms/step\n",
      "Epoch 178/200\n",
      "14/14 - 0s - loss: 0.2019 - mae: 0.2019 - val_loss: 0.2027 - val_mae: 0.2027 - 31ms/epoch - 2ms/step\n",
      "Epoch 179/200\n",
      "14/14 - 0s - loss: 0.2013 - mae: 0.2013 - val_loss: 0.2052 - val_mae: 0.2052 - 30ms/epoch - 2ms/step\n",
      "Epoch 180/200\n",
      "14/14 - 0s - loss: 0.2009 - mae: 0.2009 - val_loss: 0.2028 - val_mae: 0.2028 - 30ms/epoch - 2ms/step\n",
      "Epoch 181/200\n",
      "14/14 - 0s - loss: 0.1994 - mae: 0.1994 - val_loss: 0.2054 - val_mae: 0.2054 - 32ms/epoch - 2ms/step\n",
      "Epoch 182/200\n",
      "14/14 - 0s - loss: 0.2013 - mae: 0.2013 - val_loss: 0.2022 - val_mae: 0.2022 - 31ms/epoch - 2ms/step\n",
      "Epoch 183/200\n",
      "14/14 - 0s - loss: 0.2005 - mae: 0.2005 - val_loss: 0.2026 - val_mae: 0.2026 - 32ms/epoch - 2ms/step\n",
      "Epoch 184/200\n",
      "14/14 - 0s - loss: 0.2002 - mae: 0.2002 - val_loss: 0.2047 - val_mae: 0.2047 - 30ms/epoch - 2ms/step\n",
      "Epoch 185/200\n",
      "14/14 - 0s - loss: 0.2000 - mae: 0.2000 - val_loss: 0.2025 - val_mae: 0.2025 - 31ms/epoch - 2ms/step\n",
      "Epoch 186/200\n",
      "14/14 - 0s - loss: 0.1995 - mae: 0.1995 - val_loss: 0.2045 - val_mae: 0.2045 - 31ms/epoch - 2ms/step\n",
      "Epoch 187/200\n",
      "14/14 - 0s - loss: 0.2009 - mae: 0.2009 - val_loss: 0.2013 - val_mae: 0.2013 - 31ms/epoch - 2ms/step\n",
      "Epoch 188/200\n",
      "14/14 - 0s - loss: 0.2001 - mae: 0.2001 - val_loss: 0.2033 - val_mae: 0.2033 - 30ms/epoch - 2ms/step\n",
      "Epoch 189/200\n",
      "14/14 - 0s - loss: 0.2002 - mae: 0.2002 - val_loss: 0.2070 - val_mae: 0.2070 - 30ms/epoch - 2ms/step\n",
      "Epoch 190/200\n",
      "14/14 - 0s - loss: 0.1999 - mae: 0.1999 - val_loss: 0.2032 - val_mae: 0.2032 - 29ms/epoch - 2ms/step\n",
      "Epoch 191/200\n",
      "14/14 - 0s - loss: 0.1995 - mae: 0.1995 - val_loss: 0.2026 - val_mae: 0.2026 - 30ms/epoch - 2ms/step\n",
      "Epoch 192/200\n",
      "14/14 - 0s - loss: 0.1997 - mae: 0.1997 - val_loss: 0.2047 - val_mae: 0.2047 - 32ms/epoch - 2ms/step\n",
      "Epoch 193/200\n",
      "14/14 - 0s - loss: 0.2006 - mae: 0.2006 - val_loss: 0.2011 - val_mae: 0.2011 - 30ms/epoch - 2ms/step\n",
      "Epoch 194/200\n",
      "14/14 - 0s - loss: 0.2002 - mae: 0.2002 - val_loss: 0.2046 - val_mae: 0.2046 - 31ms/epoch - 2ms/step\n",
      "Epoch 195/200\n",
      "14/14 - 0s - loss: 0.2006 - mae: 0.2006 - val_loss: 0.2066 - val_mae: 0.2066 - 31ms/epoch - 2ms/step\n",
      "Epoch 196/200\n",
      "14/14 - 0s - loss: 0.2001 - mae: 0.2001 - val_loss: 0.2061 - val_mae: 0.2061 - 31ms/epoch - 2ms/step\n",
      "Epoch 197/200\n",
      "14/14 - 0s - loss: 0.1981 - mae: 0.1981 - val_loss: 0.2059 - val_mae: 0.2059 - 30ms/epoch - 2ms/step\n",
      "Epoch 198/200\n",
      "14/14 - 0s - loss: 0.1999 - mae: 0.1999 - val_loss: 0.2047 - val_mae: 0.2047 - 30ms/epoch - 2ms/step\n",
      "Epoch 199/200\n",
      "14/14 - 0s - loss: 0.1999 - mae: 0.1999 - val_loss: 0.2054 - val_mae: 0.2054 - 30ms/epoch - 2ms/step\n",
      "Epoch 200/200\n",
      "14/14 - 0s - loss: 0.2006 - mae: 0.2006 - val_loss: 0.2000 - val_mae: 0.2000 - 31ms/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# 单CPU or GPU版本，若有GPU则自动切换  \n",
    "model = Sequential()  # 初始化，很重要！\n",
    "model.add(Dense(units = 20,   # 输出大小  \n",
    "                activation='relu',  # 激励函数  \n",
    "                input_shape=(x_train_pd.shape[1],)  # 输入大小, 也就是列的大小  \n",
    "                ) \n",
    "         )\n",
    "\n",
    "model.add(Dropout(0.2))  # 丢弃神经元链接概率  \n",
    "\n",
    "model.add(Dense(units = 15,  \n",
    "#                 kernel_regularizer=regularizers.l2(0.01),  # 施加在权重上的正则项  \n",
    "#                 activity_regularizer=regularizers.l1(0.01),  # 施加在输出上的正则项  \n",
    "                 activation='relu', # 激励函数  \n",
    "#                  bias_regularizer=regularizers.l1_l2(0.01)  # 施加在偏置向量上的正则项  \n",
    "                ) \n",
    "         )\n",
    "\n",
    "# #######\n",
    "# model.add(Dense(units = 25,  \n",
    "# #                 kernel_regularizer=regularizers.l2(0.01),  # 施加在权重上的正则项  \n",
    "# #                 activity_regularizer=regularizers.l1(0.01),  # 施加在输出上的正则项  \n",
    "#                  activation='relu', # 激励函数  \n",
    "#                  bias_regularizer=regularizers.l1_l2(0.01)  # 施加在偏置向量上的正则项  \n",
    "#                 ) \n",
    "#          )\n",
    "# model.add(Dense(units = 35,  \n",
    "# #                 kernel_regularizer=regularizers.l2(0.01),  # 施加在权重上的正则项  \n",
    "# #                 activity_regularizer=regularizers.l1(0.01),  # 施加在输出上的正则项  \n",
    "#                  activation='relu', # 激励函数  \n",
    "#                  bias_regularizer=regularizers.l1_l2(0.01)  # 施加在偏置向量上的正则项  \n",
    "#                 ) \n",
    "#          )\n",
    "# model.add(Dense(units = 20,  \n",
    "# #                 kernel_regularizer=regularizers.l2(0.01),  # 施加在权重上的正则项  \n",
    "# #                 activity_regularizer=regularizers.l1(0.01),  # 施加在输出上的正则项  \n",
    "#                  activation='relu', # 激励函数  \n",
    "#                  bias_regularizer=regularizers.l1_l2(0.01)  # 施加在偏置向量上的正则项  \n",
    "#                 ) \n",
    "#          )\n",
    "# model.add(Dense(units = 10,  \n",
    "# #                 kernel_regularizer=regularizers.l2(0.01),  # 施加在权重上的正则项  \n",
    "# #                 activity_regularizer=regularizers.l1(0.01),  # 施加在输出上的正则项  \n",
    "#                  activation='relu', # 激励函数  \n",
    "# #                  bias_regularizer=regularizers.l1_l2(0.01)  # 施加在偏置向量上的正则项  \n",
    "#                 ) \n",
    "#          )\n",
    "\n",
    "#######\n",
    "   \n",
    "model.add(Dense(units = 2,     \n",
    "                activation='linear'  # 线性激励函数 回归一般在输出层用这个激励函数  \n",
    "               )  \n",
    "         )  \n",
    "   \n",
    "print(model.summary())  # 打印网络层次结构  \n",
    "    \n",
    "model.compile(loss='mae',  # 损失均方误差 \n",
    "              optimizer='adam',  # 优化器  \n",
    "              metrics=['mae']\n",
    "             )  \n",
    "# model.compile(optimizer='adam', loss='mse', metrics=[distance])\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200,  # 迭代次数  \n",
    "                    batch_size=200,  # 每次用来梯度下降的批处理数据大小  \n",
    "                    verbose=2,  # verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：输出训练进度，2：输出每一个epoch \n",
    "                    validation_data = (x_valid, y_valid)  # 验证集  \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "# 绘制训练 & 验证的损失值  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('Model loss')  \n",
    "plt.ylabel('Loss')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.legend(['Train', 'Test'], loc='upper left')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 557us/step\n",
      "平均绝对误差 MAE :  228.6358\n",
      "均方误差 MSE :  81502.7428\n",
      "平均绝对百分比误差 MAPE :  0.6079\n",
      "绝对误差中位数 MedAE :  192.6000\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "y_new = model.predict(x_train) \n",
    "# 反归一化还原原始量纲  \n",
    "min_max_scaler.fit(y_train_pd) \n",
    "y_new = min_max_scaler.inverse_transform(y_new) \n",
    "\n",
    "err = [distance(p,t) for p, t in zip(y_train_pd.values, y_new)]\n",
    "print('平均绝对误差 MAE : ', '%.4f'% np.mean(np.abs(err)))\n",
    "print('均方误差 MSE : ', '%.4f'% np.mean(np.square(err)))\n",
    "percentage_err = np.abs(err - np.mean(err)) / np.mean(err)\n",
    "print('平均绝对百分比误差 MAPE : ', '%.4f'% np.mean(percentage_err))\n",
    "print('绝对误差中位数 MedAE : ', '%.4f'% np.median(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 712us/step\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "y_new = model.predict(x_valid)  \n",
    "# 反归一化还原原始量纲  \n",
    "min_max_scaler.fit(y_valid_pd) \n",
    "y_new = min_max_scaler.inverse_transform(y_new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差 MAE :  240.6631\n",
      "均方误差 MSE :  83611.1121\n",
      "平均绝对百分比误差 MAPE :  0.5359\n",
      "绝对误差中位数 MedAE :  208.7500\n"
     ]
    }
   ],
   "source": [
    "err = [distance(p,t) for p, t in zip(y_valid_pd.values, y_new)]\n",
    "print('平均绝对误差 MAE : ', '%.4f'% np.mean(np.abs(err)))\n",
    "print('均方误差 MSE : ', '%.4f'% np.mean(np.square(err)))\n",
    "percentage_err = np.abs(err - np.mean(err)) / np.mean(err)\n",
    "print('平均绝对百分比误差 MAPE : ', '%.4f'% np.mean(percentage_err))\n",
    "print('绝对误差中位数 MedAE : ', '%.4f'% np.median(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差 MAE :  238.5221\n",
      "均方误差 MSE :  78712.5512\n",
      "平均绝对百分比误差 MAPE :  0.4923\n",
      "绝对误差中位数 MedAE :  220.2000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差 MAE :  245.0520\n",
      "均方误差 MSE :  81915.8621\n",
      "平均绝对百分比误差 MAPE :  0.4848\n",
      "绝对误差中位数 MedAE :  213.6500\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差 MAE :  247.3101\n",
      "均方误差 MSE :  84211.9479\n",
      "平均绝对百分比误差 MAPE :  0.4903\n",
      "绝对误差中位数 MedAE :  224.2500\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差 MAE :  236.4016\n",
      "均方误差 MSE :  81168.3152\n",
      "平均绝对百分比误差 MAPE :  0.5355\n",
      "绝对误差中位数 MedAE :  204.0000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差 MAE :  241.3319\n",
      "均方误差 MSE :  79592.4548\n",
      "平均绝对百分比误差 MAPE :  0.4692\n",
      "绝对误差中位数 MedAE :  216.7500\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差 MAE :  244.8089\n",
      "均方误差 MSE :  82334.4263\n",
      "平均绝对百分比误差 MAPE :  0.4796\n",
      "绝对误差中位数 MedAE :  222.6500\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差 MAE :  254.4007\n",
      "均方误差 MSE :  81825.0678\n",
      "平均绝对百分比误差 MAPE :  0.4066\n",
      "绝对误差中位数 MedAE :  246.4500\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差 MAE :  263.2424\n",
      "均方误差 MSE :  94925.1076\n",
      "平均绝对百分比误差 MAPE :  0.4721\n",
      "绝对误差中位数 MedAE :  228.4000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.4952117   31.28495181] [121.49686   31.284838]\n",
      "[121.4973961  31.285062 ] [121.49835   31.284945]\n",
      "[121.4960706   31.28294881] [121.49702   31.284853]\n",
      "[121.5009587   31.28556232] [121.499565  31.285448]\n",
      "[121.4934954   31.28619217] [121.496254  31.283316]\n",
      "[121.4935116   31.28620841] [121.49731   31.285402]\n",
      "[121.4960706   31.28294881] [121.49718   31.284746]\n",
      "[121.4960706   31.28294881] [121.496254  31.283718]\n",
      "[121.5027337   31.28383628] [121.49966   31.285503]\n",
      "[121.4952597   31.28490205] [121.50041   31.285767]\n",
      "[121.4960706   31.28294881] [121.49609   31.282862]\n",
      "[121.4976305   31.28639133] [121.49865   31.285336]\n",
      "[121.4940567   31.28409229] [121.496254  31.28434 ]\n",
      "[121.4939158   31.28453393] [121.49626   31.284513]\n",
      "[121.4940567   31.28409229] [121.4964   31.28428]\n",
      "[121.4958011   31.28309687] [121.49683   31.284544]\n",
      "[121.4994009   31.28594976] [121.49869   31.284012]\n",
      "[121.5014033  31.2851369] [121.4964    31.283958]\n",
      "[121.4927527   31.28434635] [121.49663  31.28446]\n",
      "[121.4954682   31.28620791] [121.499176  31.285427]\n",
      "[121.4960494   31.28294037] [121.49648   31.284012]\n",
      "[121.4966909   31.28661194] [121.496666  31.284403]\n",
      "[121.4952794   31.28377062] [121.49658   31.284742]\n",
      "[121.4935505   31.28427968] [121.49632   31.284723]\n",
      "[121.4966909   31.28661194] [121.496124  31.284534]\n",
      "[121.4960706   31.28294881] [121.49663   31.284718]\n",
      "[121.5018041   31.28614234] [121.49831   31.286013]\n",
      "[121.5023904   31.28621388] [121.50023   31.284472]\n",
      "[121.4935505   31.28427968] [121.49715  31.28474]\n",
      "[121.5023498   31.28396201] [121.50082   31.284916]\n",
      "[121.5002939   31.28554227] [121.4969    31.285604]\n",
      "[121.4960706   31.28294881] [121.49607   31.284552]\n",
      "[121.4963968   31.28344556] [121.49715   31.284708]\n",
      "[121.4962805   31.28329606] [121.496254  31.283293]\n",
      "[121.4954682   31.28620791] [121.49814  31.28578]\n",
      "[121.4960706   31.28294881] [121.496376  31.284649]\n",
      "[121.4949788   31.28350233] [121.4964   31.28439]\n",
      "[121.4952042   31.28499461] [121.49729   31.284956]\n",
      "[121.4935505   31.28427968] [121.49615   31.284567]\n",
      "[121.4953794   31.28332017] [121.49753   31.285498]\n",
      "[121.4952597   31.28490205] [121.496414  31.284912]\n",
      "[121.4960706   31.28294881] [121.496056  31.283031]\n",
      "[121.4963968   31.28344556] [121.49803   31.284863]\n",
      "[121.4966909   31.28661194] [121.4968    31.285631]\n",
      "[121.4994009   31.28594976] [121.49599  31.28457]\n",
      "[121.4971518   31.28448703] [121.49655   31.284601]\n",
      "[121.4937911  31.2846796] [121.49599  31.28457]\n",
      "[121.4960706   31.28294881] [121.498375  31.285112]\n",
      "[121.4960706   31.28294881] [121.496124  31.283274]\n",
      "[121.4953873   31.28328944] [121.49968  31.28421]\n",
      "[121.4963968   31.28344556] [121.496346  31.284529]\n",
      "[121.5001968   31.28565588] [121.49895  31.28502]\n",
      "[121.4954682   31.28620791] [121.49901   31.285412]\n",
      "[121.5004456   31.28630842] [121.4978    31.285225]\n",
      "[121.4966909   31.28661194] [121.49676   31.285643]\n",
      "[121.4954463   31.28363835] [121.49966   31.285507]\n",
      "[121.4973961  31.285062 ] [121.49832   31.285173]\n",
      "[121.5024289   31.28405776] [121.50133   31.285057]\n",
      "[121.501712    31.28611143] [121.49789   31.285105]\n",
      "[121.4929114   31.28530074] [121.49603  31.28457]\n",
      "[121.5023074   31.28559868] [121.49632   31.284515]\n",
      "[121.493794    31.28467891] [121.496185  31.28457 ]\n",
      "[121.501852    31.28605005] [121.49735   31.284723]\n",
      "[121.5024807   31.28415768] [121.49607   31.284153]\n",
      "[121.4940567   31.28409229] [121.49614   31.284668]\n",
      "[121.4933914   31.28437069] [121.497826  31.284872]\n",
      "[121.4954404   31.28350973] [121.49856   31.284967]\n",
      "[121.4930318   31.28431022] [121.49679   31.284441]\n",
      "[121.4925544   31.28431855] [121.49727  31.28471]\n",
      "[121.4953808   31.28333751] [121.49616   31.284582]\n",
      "[121.4990673   31.28449594] [121.49644   31.284006]\n",
      "[121.4960706   31.28294881] [121.49786   31.283564]\n",
      "[121.496297    31.28354031] [121.49632   31.283472]\n",
      "[121.4938612   31.28458231] [121.496284  31.284418]\n",
      "[121.4940567   31.28409229] [121.49713   31.284689]\n",
      "[121.4963583   31.28441532] [121.496445  31.284079]\n",
      "[121.4963968   31.28344556] [121.4991    31.285292]\n",
      "[121.496297    31.28354031] [121.49632   31.283552]\n",
      "[121.497739    31.28642425] [121.497086  31.284906]\n",
      "[121.4966909   31.28661194] [121.49731   31.284584]\n",
      "[121.496297    31.28354031] [121.49665   31.284626]\n",
      "[121.4935505   31.28427968] [121.49676  31.28457]\n",
      "[121.4960706   31.28294881] [121.49617  31.28369]\n",
      "[121.4925544   31.28431855] [121.49696   31.284666]\n",
      "[121.4963968   31.28344556] [121.501396  31.28571 ]\n",
      "[121.4960706   31.28294881] [121.49621   31.284618]\n",
      "[121.496297    31.28354031] [121.496185  31.28413 ]\n",
      "[121.4960706   31.28294881] [121.49619   31.283226]\n",
      "[121.4960706   31.28294881] [121.49786   31.283564]\n",
      "[121.5013504   31.28496175] [121.497574  31.284676]\n",
      "[121.5005391   31.28639489] [121.49825   31.285093]\n",
      "[121.4960706   31.28294881] [121.496185  31.283407]\n",
      "[121.4952597   31.28490205] [121.49784   31.285069]\n",
      "[121.5027587   31.28376769] [121.49857   31.285086]\n",
      "[121.4954315   31.28350182] [121.499146  31.285477]\n",
      "[121.4935505   31.28427968] [121.497826  31.284927]\n",
      "[121.4967785  31.2865537] [121.49719   31.285233]\n",
      "[121.4929114   31.28530074] [121.499245  31.285618]\n",
      "[121.5032871   31.28364229] [121.49826  31.28484]\n",
      "[121.5022967   31.28556065] [121.49617   31.283997]\n",
      "[121.502038    31.28584426] [121.49832   31.285091]\n",
      "[121.4938321   31.28457832] [121.49626   31.284513]\n",
      "[121.4960706   31.28294881] [121.49621   31.283064]\n",
      "[121.5035904   31.28366566] [121.50075  31.28583]\n",
      "[121.4994009   31.28594976] [121.49616   31.284575]\n",
      "[121.5027966   31.28556459] [121.49616   31.282982]\n",
      "[121.5016176   31.28560266] [121.49681   31.284378]\n",
      "[121.4994009   31.28594976] [121.49631  31.28466]\n",
      "[121.5033092   31.28361913] [121.49655  31.28417]\n",
      "[121.4960706   31.28294881] [121.49777   31.284828]\n",
      "[121.4960706   31.28294881] [121.49699  31.28489]\n",
      "[121.5013504   31.28496175] [121.4964   31.28459]\n",
      "[121.5011044   31.28637026] [121.500824  31.284977]\n",
      "[121.4960706   31.28294881] [121.496185  31.283092]\n",
      "[121.502463    31.28409765] [121.50087   31.284681]\n",
      "[121.4971439   31.28451915] [121.4972   31.28491]\n",
      "[121.4960698   31.28294531] [121.49637   31.284306]\n",
      "[121.4960706   31.28294881] [121.49626   31.283485]\n",
      "[121.4957781   31.28356981] [121.4981    31.285353]\n",
      "[121.5019656   31.28632329] [121.49832   31.285866]\n",
      "[121.4973961  31.285062 ] [121.49667   31.284616]\n",
      "[121.5023926   31.28531499] [121.49623   31.283648]\n",
      "[121.4960706   31.28294881] [121.49614   31.284624]\n",
      "[121.4935089   31.28633221] [121.49745   31.284746]\n",
      "[121.5021622   31.28611677] [121.50032  31.28491]\n",
      "[121.4952597   31.28490205] [121.499054  31.285444]\n",
      "[121.4935505   31.28427968] [121.49719  31.28478]\n",
      "[121.4966909   31.28661194] [121.496216  31.283789]\n",
      "[121.5018874   31.28609313] [121.49784   31.285738]\n",
      "[121.4939631   31.28436407] [121.497154  31.28491 ]\n",
      "[121.5002799   31.28557576] [121.49856   31.284967]\n",
      "[121.4954764   31.28352778] [121.49832   31.285143]\n",
      "[121.4971527   31.28452453] [121.49717  31.28546]\n",
      "[121.4939178   31.28458576] [121.49621   31.284576]\n",
      "[121.4960706   31.28294881] [121.49616   31.283442]\n",
      "[121.5021895   31.28577941] [121.499855  31.284325]\n",
      "[121.4950356   31.28346117] [121.49616   31.284115]\n",
      "[121.4953292  31.2833464] [121.496925  31.285131]\n",
      "[121.4935505   31.28427968] [121.49609   31.284592]\n",
      "[121.4960706   31.28294881] [121.496346  31.283203]\n",
      "[121.4953314   31.28629498] [121.49798   31.285164]\n",
      "[121.4935505   31.28427968] [121.499374  31.285452]\n",
      "[121.4952158   31.28496072] [121.49642   31.284418]\n",
      "[121.5006329   31.28617237] [121.49837   31.285254]\n",
      "[121.4929114   31.28530074] [121.497246  31.28463 ]\n",
      "[121.4994009   31.28594976] [121.4967    31.284342]\n",
      "[121.4963968   31.28344556] [121.501465  31.285719]\n",
      "[121.4960706   31.28294881] [121.49755   31.284985]\n",
      "[121.4971413   31.28451763] [121.49798   31.285173]\n",
      "[121.4994009   31.28594976] [121.49658   31.284428]\n",
      "[121.4943905   31.28665237] [121.496765  31.285639]\n",
      "[121.5005504   31.28639142] [121.49807   31.285173]\n",
      "[121.5021545   31.28609341] [121.500244  31.284868]\n",
      "[121.4986547   31.28459074] [121.50111   31.285656]\n",
      "[121.4952597   31.28490205] [121.50041   31.285767]\n",
      "[121.4960706   31.28294881] [121.499084  31.285292]\n",
      "[121.4954682   31.28620791] [121.4997    31.285643]\n",
      "[121.4960706   31.28294881] [121.49626   31.283485]\n",
      "[121.4971706   31.28453563] [121.49717  31.28546]\n",
      "[121.4976417   31.28638534] [121.49897   31.285477]\n",
      "[121.4960706   31.28294881] [121.49617   31.284674]\n",
      "[121.4960706   31.28294881] [121.49603   31.283062]\n",
      "[121.4976417   31.28638534] [121.49745   31.284767]\n",
      "[121.4960706   31.28294881] [121.49644  31.28344]\n",
      "[121.4934876   31.28626869] [121.49755   31.284723]\n",
      "[121.5021065   31.28578648] [121.499855  31.284325]\n",
      "[121.5023385   31.28397035] [121.49751   31.285019]\n",
      "[121.4942999   31.28665804] [121.49681   31.284645]\n",
      "[121.5021401   31.28593695] [121.500336  31.28581 ]\n",
      "[121.4973489   31.28456368] [121.4971    31.284908]\n",
      "[121.4935505   31.28427968] [121.49873   31.285381]\n",
      "[121.5013504   31.28496175] [121.498505  31.284943]\n",
      "[121.4960706   31.28294881] [121.49812   31.285112]\n",
      "[121.4973961  31.285062 ] [121.496284  31.284145]\n",
      "[121.5018817   31.28605791] [121.49784   31.285738]\n",
      "[121.496297    31.28354031] [121.49658  31.28404]\n",
      "[121.4960706   31.28294881] [121.49663   31.284393]\n",
      "[121.5030084   31.28371634] [121.49835  31.28512]\n",
      "[121.4954682   31.28620791] [121.4963    31.284388]\n",
      "[121.5021877  31.2862332] [121.50064   31.285828]\n",
      "[121.4973961  31.285062 ] [121.4975    31.284843]\n",
      "[121.4961035   31.28354653] [121.496254  31.283606]\n",
      "[121.4925505  31.2842825] [121.49694   31.284557]\n",
      "[121.4994009   31.28594976] [121.497086  31.28461 ]\n",
      "[121.4952293  31.284947 ] [121.50256   31.284353]\n",
      "[121.496059    31.28294437] [121.49667   31.284508]\n",
      "[121.4963968   31.28344556] [121.49782   31.285131]\n",
      "[121.4973287   31.28466044] [121.497665  31.284834]\n",
      "[121.4938224   31.28464827] [121.49616  31.2846 ]\n",
      "[121.5001944   31.28559394] [121.49616   31.284615]\n",
      "[121.4960706   31.28294881] [121.49612   31.284645]\n",
      "[121.4953381   31.28508333] [121.49751   31.284973]\n",
      "[121.4960706   31.28294881] [121.497665  31.28482 ]\n",
      "[121.4935505   31.28427968] [121.49616   31.284615]\n",
      "[121.4950625   31.28343706] [121.496254  31.283682]\n",
      "[121.5042566   31.28631703] [121.49624   31.283562]\n",
      "[121.5006329   31.28617237] [121.49835   31.285023]\n",
      "[121.5005191  31.2862859] [121.498375  31.285145]\n",
      "[121.4960706   31.28294881] [121.4964    31.284437]\n",
      "[121.4973961  31.285062 ] [121.49632   31.284723]\n",
      "[121.5026421   31.28468929] [121.50172   31.284378]\n",
      "[121.4953873   31.28328944] [121.49983  31.28431]\n",
      "[121.496297    31.28354031] [121.496506  31.2847  ]\n",
      "[121.5026421   31.28468929] [121.50075   31.284517]\n",
      "[121.5005449  31.286404 ] [121.49632   31.284723]\n",
      "[121.4966909   31.28661194] [121.49647   31.284672]\n",
      "[121.4994009   31.28594976] [121.49631  31.28466]\n",
      "[121.4966909   31.28661194] [121.49614   31.283852]\n",
      "[121.4948963  31.2864568] [121.496765  31.285639]\n",
      "[121.5034259   31.28369453] [121.497574  31.284592]\n",
      "[121.5016587   31.28562167] [121.4984    31.285091]\n",
      "[121.5027909  31.2837575] [121.49906   31.285404]\n",
      "[121.5005665   31.28610338] [121.49812   31.285112]\n",
      "[121.4943646   31.28666052] [121.49614   31.283413]\n",
      "[121.4960706   31.28294881] [121.49665   31.284374]\n",
      "[121.4992484   31.28598341] [121.49631  31.28466]\n",
      "[121.4954682   31.28620791] [121.49633   31.284262]\n",
      "[121.4960706   31.28294881] [121.49616   31.283512]\n",
      "[121.4994009   31.28594976] [121.49619   31.284582]\n",
      "[121.4963968   31.28344556] [121.49628   31.283367]\n",
      "[121.496297    31.28354031] [121.49616   31.283092]\n",
      "[121.4960706   31.28294881] [121.496056  31.284615]\n",
      "[121.4963968   31.28344556] [121.49708   31.284864]\n",
      "[121.4940567   31.28409229] [121.49692   31.284641]\n",
      "[121.4951995   31.28496969] [121.497215  31.284885]\n",
      "[121.4952597   31.28490205] [121.49974   31.284046]\n",
      "[121.5019549   31.28586981] [121.4984    31.284986]\n",
      "[121.4990176   31.28558372] [121.4968    31.285631]\n",
      "[121.4960706   31.28294881] [121.49719  31.28465]\n",
      "[121.496297    31.28354031] [121.49679   31.285635]\n",
      "[121.4994009   31.28594976] [121.498505  31.283676]\n",
      "[121.4935505   31.28427968] [121.49696   31.284586]\n",
      "[121.4958376  31.2832611] [121.49715   31.285505]\n",
      "[121.4963968   31.28344556] [121.49609  31.28442]\n",
      "[121.4943483   31.28667006] [121.496124  31.28405 ]\n",
      "[121.497341    31.28454168] [121.496445  31.28468 ]\n",
      "[121.4954638   31.28364082] [121.498375  31.285187]\n",
      "[121.493794    31.28467891] [121.496185  31.28457 ]\n",
      "[121.4939693   31.28444515] [121.49686   31.284853]\n",
      "[121.4960706   31.28294881] [121.496254  31.283499]\n",
      "[121.4994009   31.28594976] [121.496     31.284573]\n",
      "[121.4960706   31.28294881] [121.4968    31.284573]\n",
      "[121.4966909   31.28661194] [121.49632   31.284473]\n",
      "[121.4966909   31.28661194] [121.49722   31.284967]\n",
      "[121.502038    31.28608912] [121.497055  31.285257]\n",
      "[121.5027909  31.2837575] [121.49906   31.285404]\n",
      "[121.4954193   31.28363279] [121.49862   31.285387]\n",
      "[121.4963968   31.28344556] [121.496346  31.283968]\n",
      "[121.4932649   31.28584835] [121.4963   31.28423]\n",
      "[121.5013504   31.28496175] [121.4964   31.28459]\n",
      "[121.4925671   31.28433877] [121.496414  31.284752]\n",
      "[121.4973961  31.285062 ] [121.49663   31.284365]\n",
      "[121.4960094   31.28353797] [121.49624   31.283556]\n",
      "[121.4960706   31.28294881] [121.496185  31.283285]\n",
      "[121.5023648  31.2840115] [121.50138  31.2852 ]\n",
      "[121.502463    31.28409765] [121.50087   31.284681]\n",
      "[121.5002799   31.28557576] [121.49874   31.284988]\n",
      "[121.4960706   31.28294881] [121.49621   31.284624]\n",
      "[121.5025347   31.28413839] [121.49933   31.283848]\n",
      "[121.4960706   31.28294881] [121.49619   31.283243]\n",
      "[121.5015501   31.28627179] [121.49807  31.28503]\n",
      "[121.4952159   31.28496005] [121.498184  31.285141]\n",
      "[121.4940567   31.28409229] [121.49753   31.284777]\n",
      "[121.4960706   31.28294881] [121.49637   31.283886]\n",
      "[121.4925627   31.28429641] [121.49696   31.284666]\n",
      "[121.4960706   31.28294881] [121.49616   31.284548]\n",
      "[121.4954682   31.28620791] [121.49687   31.284803]\n",
      "[121.4951979   31.28496048] [121.49747   31.285069]\n",
      "[121.4960706   31.28294881] [121.49599   31.284588]\n",
      "[121.4929114   31.28530074] [121.49685  31.28447]\n",
      "[121.5016558   31.28562658] [121.49664   31.284101]\n",
      "[121.4992703   31.28598287] [121.4963    31.284657]\n",
      "[121.4938297   31.28462985] [121.49628   31.284649]\n",
      "[121.5013504   31.28496175] [121.498276  31.285152]\n",
      "[121.4960706   31.28294881] [121.49624   31.284203]\n",
      "[121.496297    31.28354031] [121.49632  31.28367]\n",
      "[121.4960706   31.28294881] [121.4961    31.284557]\n",
      "[121.4960706   31.28294881] [121.496216  31.283075]\n",
      "[121.4960706   31.28294881] [121.49623  31.28393]\n",
      "[121.4935505   31.28427968] [121.49943  31.28549]\n",
      "[121.4973961  31.285062 ] [121.496376  31.284267]\n",
      "[121.5024477  31.2863657] [121.50381   31.286642]\n",
      "[121.5021065   31.28578648] [121.49977   31.284323]\n",
      "[121.4958011   31.28309687] [121.496506  31.284494]\n",
      "[121.5011081   31.28636959] [121.501045  31.285126]\n",
      "[121.5013504   31.28496175] [121.49632  31.28421]\n",
      "[121.5027509   31.28382041] [121.4991    31.285503]\n",
      "[121.4948159   31.28645581] [121.49671   31.285656]\n",
      "[121.4994009   31.28594976] [121.496124  31.284582]\n",
      "[121.4985683   31.28454456] [121.49931   31.285406]\n",
      "[121.4960706   31.28294881] [121.496994  31.284546]\n",
      "[121.497126    31.28646993] [121.49738   31.284943]\n",
      "[121.4925641   31.28433052] [121.496414  31.284752]\n",
      "[121.4994009   31.28594976] [121.49616   31.284575]\n",
      "[121.4960706   31.28294881] [121.496185  31.283285]\n",
      "[121.4960706   31.28294881] [121.496124  31.283274]\n",
      "[121.4966909   31.28661194] [121.4968    31.285631]\n",
      "[121.4949751   31.28360763] [121.49649   31.284462]\n",
      "[121.4955847   31.28329615] [121.4967    31.284954]\n",
      "[121.5024477  31.2863657] [121.501854  31.286186]\n",
      "[121.4963528   31.28338174] [121.49624   31.283268]\n",
      "[121.495456   31.2835592] [121.49743   31.284632]\n",
      "[121.496297    31.28354031] [121.49616   31.284557]\n",
      "[121.4960706   31.28294881] [121.49632   31.283548]\n",
      "[121.4935505   31.28427968] [121.49778   31.284876]\n",
      "[121.4960706   31.28294881] [121.49612   31.283882]\n",
      "[121.4949775   31.28347577] [121.49726   31.285475]\n",
      "[121.4963968   31.28344556] [121.4964    31.284061]\n",
      "[121.496297    31.28354031] [121.49628   31.283943]\n",
      "[121.4960592   31.28294483] [121.49628  31.28364]\n",
      "[121.5013504   31.28496175] [121.49736   31.284676]\n",
      "[121.4960706   31.28294881] [121.49619   31.284088]\n",
      "[121.4960706   31.28294881] [121.49651   31.283476]\n",
      "[121.5016176   31.28560266] [121.49681   31.284378]\n",
      "[121.4937939  31.2846789] [121.496254  31.284575]\n",
      "[121.4963968   31.28344556] [121.496994  31.284756]\n",
      "[121.4940567   31.28409229] [121.49616  31.28369]\n",
      "[121.4966909   31.28661194] [121.49697  31.28557]\n",
      "[121.4960706   31.28294881] [121.496185  31.28339 ]\n",
      "[121.4954711   31.28355262] [121.497734  31.284798]\n",
      "[121.4951949   31.28499217] [121.49798   31.285164]\n",
      "[121.4960706   31.28294881] [121.49617   31.283579]\n",
      "[121.501656   31.2856277] [121.49678  31.28443]\n",
      "[121.5009333   31.28563502] [121.50055   31.285496]\n",
      "[121.5032871   31.28364229] [121.49826  31.28484]\n",
      "[121.4938443   31.28455312] [121.49614  31.28303]\n",
      "[121.4960706   31.28294881] [121.496185  31.283152]\n",
      "[121.5019428   31.28608435] [121.49874   31.285263]\n",
      "[121.4975311   31.28640438] [121.49906  31.28529]\n",
      "[121.5033977  31.2836985] [121.498375  31.285275]\n",
      "[121.496297    31.28354031] [121.49616   31.283016]\n",
      "[121.4981647   31.28473908] [121.49777   31.285109]\n",
      "[121.5005502   31.28639038] [121.49766   31.284899]\n",
      "[121.4960601   31.28294487] [121.49628  31.28366]\n",
      "[121.4935505   31.28427968] [121.49733   31.284698]\n",
      "[121.4960706   31.28294881] [121.496124  31.283056]\n",
      "[121.4960706   31.28294881] [121.496124  31.283098]\n",
      "[121.4973961  31.285062 ] [121.49663   31.284365]\n",
      "[121.4957781   31.28356981] [121.49755  31.28481]\n",
      "[121.4954773   31.28328642] [121.49633   31.284344]\n",
      "[121.4960706   31.28294881] [121.49699  31.28489]\n",
      "[121.4963968   31.28344556] [121.49624   31.283463]\n",
      "[121.4994009   31.28594976] [121.49623  31.28463]\n",
      "[121.4985683   31.28454456] [121.49931   31.285406]\n",
      "[121.4932758   31.28588549] [121.496254  31.284088]\n",
      "[121.5004599   31.28605797] [121.497475  31.285343]\n",
      "[121.4966909   31.28661194] [121.49679   31.285635]\n",
      "[121.4960706   31.28294881] [121.49639   31.283335]\n",
      "[121.4960706   31.28294881] [121.49616  31.28297]\n",
      "[121.4963968   31.28344556] [121.49676   31.284561]\n",
      "[121.4937941   31.28467893] [121.496284  31.284418]\n",
      "[121.4960706   31.28294881] [121.49883  31.28495]\n",
      "[121.5022421  31.2855159] [121.49648   31.284384]\n",
      "[121.5031177   31.28376034] [121.497795  31.284914]\n",
      "[121.4952117   31.28495181] [121.49753   31.284729]\n",
      "[121.4954682   31.28620791] [121.49718   31.284737]\n",
      "[121.4966909   31.28661194] [121.49616   31.284204]\n",
      "[121.496297    31.28354031] [121.4964    31.284235]\n",
      "[121.4970708  31.2864784] [121.49919   31.285515]\n",
      "[121.4938319   31.28457841] [121.49624   31.284548]\n",
      "[121.5023926   31.28531499] [121.49649   31.284962]\n",
      "[121.5013504   31.28496175] [121.4977    31.284956]\n",
      "[121.4963968   31.28344556] [121.4964    31.284061]\n",
      "[121.4966909   31.28661194] [121.496765  31.285639]\n",
      "[121.500202    31.28571357] [121.496765  31.285639]\n",
      "[121.5024801   31.28418163] [121.49903   31.283829]\n",
      "[121.5013504   31.28496175] [121.49681  31.28486]\n",
      "[121.4960706   31.28294881] [121.496284  31.28345 ]\n",
      "[121.4960706   31.28294881] [121.498436  31.285213]\n",
      "[121.4953824   31.28330233] [121.496826  31.284813]\n",
      "[121.4960706   31.28294881] [121.49626   31.283485]\n",
      "[121.4952597   31.28490205] [121.49814   31.284992]\n",
      "[121.4960606   31.28294557] [121.49648   31.284384]\n",
      "[121.501712    31.28611143] [121.49656   31.284033]\n",
      "[121.5018645   31.28613255] [121.498505  31.28609 ]\n",
      "[121.5002075   31.28568347] [121.49706   31.285515]\n",
      "[121.50051    31.2849864] [121.49706   31.285515]\n",
      "[121.5024684  31.2841487] [121.50086  31.28501]\n",
      "[121.5015345   31.28573327] [121.50408  31.28558]\n",
      "[121.4960706   31.28294881] [121.49837   31.285082]\n",
      "[121.496297    31.28354031] [121.496284  31.28451 ]\n",
      "[121.5022423   31.28541278] [121.49624   31.283937]\n",
      "[121.5018117   31.28612399] [121.49651   31.283997]\n",
      "[121.4935505   31.28427968] [121.49655   31.284725]\n",
      "[121.502525    31.28424544] [121.50059   31.284191]\n",
      "[121.5002075   31.28568347] [121.49683   31.285622]\n",
      "[121.5002231   31.28579245] [121.4968    31.285631]\n",
      "[121.4950855   31.28343024] [121.4963   31.28341]\n",
      "[121.4938999   31.28458989] [121.49621   31.284576]\n",
      "[121.4960706   31.28294881] [121.496254  31.28464 ]\n",
      "[121.4983929   31.28467136] [121.49855  31.28516]\n",
      "[121.496297    31.28354031] [121.49639   31.284885]\n",
      "[121.4963968   31.28344556] [121.4978    31.285017]\n",
      "[121.4960706   31.28294881] [121.4997    31.284199]\n",
      "[121.4935089   31.28633221] [121.49816   31.285164]\n",
      "[121.5025797   31.28426685] [121.499565  31.28389 ]\n",
      "[121.4973961  31.285062 ] [121.4967    31.284662]\n",
      "[121.4960706   31.28294881] [121.49614   31.284086]\n",
      "[121.5004486   31.28625488] [121.49644  31.28473]\n",
      "[121.4961035   31.28354653] [121.496254  31.28358 ]\n",
      "[121.4960706   31.28294881] [121.496346  31.284626]\n",
      "[121.4960706   31.28294881] [121.496185  31.282988]\n",
      "[121.4966909   31.28661194] [121.49614   31.284147]\n",
      "[121.4994009   31.28594976] [121.49616   31.284575]\n",
      "[121.4963968   31.28344556] [121.49694  31.28455]\n",
      "[121.4954615   31.28329798] [121.4996    31.284111]\n",
      "[121.5018645   31.28613255] [121.499245  31.2856  ]\n",
      "[121.4963968   31.28344556] [121.49624  31.28332]\n",
      "[121.4963968   31.28344556] [121.49624  31.28332]\n",
      "[121.496297    31.28354031] [121.49628  31.28464]\n",
      "[121.500395    31.28521637] [121.4968    31.285631]\n",
      "[121.4951949   31.28499217] [121.49798   31.285164]\n",
      "[121.4980722   31.28477581] [121.49752   31.284746]\n",
      "[121.4952597   31.28490205] [121.49847   31.285267]\n",
      "[121.4994009   31.28594976] [121.498116  31.284008]\n",
      "[121.5025797   31.28426685] [121.4995    31.283882]\n",
      "[121.5024684  31.2841487] [121.50095  31.28477]\n",
      "[121.4943258   31.28667205] [121.49624  31.28441]\n",
      "[121.4960706   31.28294881] [121.496124  31.283285]\n",
      "[121.4962467   31.28353581] [121.49615   31.283707]\n",
      "[121.5017952   31.28609274] [121.49826   31.286026]\n",
      "[121.4982955   31.28469777] [121.49966   31.285542]\n",
      "[121.4976112   31.28639912] [121.4983   31.28496]\n",
      "[121.4943039   31.28666096] [121.49681   31.284645]\n",
      "[121.4952597   31.28490205] [121.497406  31.284859]\n",
      "[121.4935505   31.28427968] [121.49626   31.284332]\n",
      "[121.4931581   31.28564936] [121.49617   31.284636]\n",
      "[121.5024807   31.28415768] [121.49607   31.284153]\n",
      "[121.5022104   31.28600549] [121.49967  31.28544]\n",
      "[121.4973961  31.285062 ] [121.49835   31.284945]\n",
      "[121.5020009   31.28637749] [121.49615   31.283665]\n",
      "[121.4973961  31.285062 ] [121.49735  31.28485]\n",
      "[121.4978491   31.28487562] [121.49995  31.28548]\n",
      "[121.4960706   31.28294881] [121.496185  31.283285]\n",
      "[121.4935073   31.28431877] [121.49777   31.284876]\n",
      "[121.4971385   31.28449829] [121.4978    31.285225]\n",
      "[121.4934941   31.28621359] [121.497475  31.285343]\n",
      "[121.4954103   31.28366651] [121.49894  31.28541]\n",
      "[121.4961359   31.28354868] [121.49626   31.283625]\n",
      "[121.5026421   31.28468929] [121.50129   31.284454]\n",
      "[121.4994222   31.28521149] [121.498604  31.28483 ]\n",
      "[121.4973961  31.285062 ] [121.49607   31.284552]\n",
      "[121.4925671   31.28433877] [121.49757   31.284878]\n",
      "[121.496297    31.28354031] [121.49722   31.284456]\n",
      "[121.4960706   31.28294881] [121.49614   31.283957]\n",
      "[121.4994009   31.28594976] [121.497154  31.284643]\n",
      "[121.4960706   31.28294881] [121.49663   31.284628]\n",
      "[121.5025211   31.28422248] [121.501945  31.284826]\n",
      "[121.5035997   31.28369744] [121.49972   31.284048]\n",
      "[121.5026421   31.28468929] [121.50147   31.284418]\n",
      "[121.4960706   31.28294881] [121.49616  31.28336]\n",
      "[121.5022196   31.28597746] [121.49922   31.285353]\n",
      "[121.4980924   31.28476631] [121.496826  31.28461 ]\n",
      "[121.4960706   31.28294881] [121.49686   31.284725]\n",
      "[121.5013504   31.28496175] [121.49986   31.283978]\n",
      "[121.5021899   31.28609754] [121.4981    31.285143]\n",
      "[121.4950086  31.2864222] [121.49697  31.28557]\n",
      "[121.496297    31.28354031] [121.496735  31.28486 ]\n",
      "[121.4966909   31.28661194] [121.49621   31.284605]\n",
      "[121.4960706   31.28294881] [121.50131  31.28448]\n",
      "[121.4960706   31.28294881] [121.49621  31.2832 ]\n",
      "[121.5002799   31.28557576] [121.49865   31.284977]\n",
      "[121.4973961  31.285062 ] [121.49646   31.284224]\n",
      "[121.496297    31.28354031] [121.49616   31.284557]\n",
      "[121.5001968   31.28565588] [121.49917   31.285488]\n",
      "[121.4960706   31.28294881] [121.49617   31.283642]\n",
      "[121.5023418   31.28395927] [121.4966    31.284393]\n",
      "[121.5013504   31.28496175] [121.4968    31.284838]\n",
      "[121.4960706   31.28294881] [121.49614   31.283043]\n",
      "[121.49605     31.28295314] [121.49633   31.284184]\n",
      "[121.4994009   31.28594976] [121.49631  31.28466]\n",
      "[121.5013504   31.28496175] [121.49614   31.284624]\n",
      "[121.4956069   31.28327845] [121.49687   31.285143]\n",
      "[121.5001968   31.28565588] [121.49812   31.285112]\n",
      "[121.5021049   31.28579822] [121.49959   31.284254]\n",
      "[121.500468    31.28619735] [121.49605   31.284628]\n",
      "[121.4960706   31.28294881] [121.497055  31.28453 ]\n",
      "[121.4960706   31.28294881] [121.49617   31.283642]\n",
      "[121.4950455   31.28343059] [121.496414  31.284445]\n",
      "[121.4942361   31.28680627] [121.496376  31.284668]\n",
      "[121.495003    31.28346492] [121.49697  31.28557]\n",
      "[121.4953381   31.28508333] [121.49751   31.284973]\n",
      "[121.4960706   31.28294881] [121.49619   31.283514]\n",
      "[121.4960597   31.28294492] [121.49616   31.284624]\n",
      "[121.5030084   31.28371634] [121.49727   31.285198]\n",
      "[121.4966909   31.28661194] [121.49679   31.284779]\n",
      "[121.4960706   31.28294881] [121.49596   31.284592]\n",
      "[121.4966909   31.28661194] [121.49755   31.285114]\n",
      "[121.4938319   31.28457841] [121.49624   31.284548]\n",
      "[121.496297    31.28354031] [121.49631  31.28353]\n",
      "[121.4960706   31.28294881] [121.49626   31.283485]\n",
      "[121.4966909   31.28661194] [121.49686   31.284582]\n",
      "[121.4960706   31.28294881] [121.49722   31.285362]\n",
      "[121.5024633   31.28411344] [121.49761   31.284798]\n",
      "[121.495598    31.28363875] [121.49635   31.284492]\n",
      "[121.4935505   31.28427968] [121.49778   31.284876]\n",
      "[121.5035925   31.28371119] [121.49851  31.28518]\n",
      "[121.4960637   31.28294803] [121.49665   31.284224]\n",
      "[121.50051    31.2849864] [121.496025  31.284569]\n",
      "[121.4950921   31.28639743] [121.49717  31.28546]\n",
      "[121.4960706   31.28294881] [121.49631   31.283724]\n",
      "[121.5013776  31.2860664] [121.496666  31.284336]\n",
      "[121.4966909   31.28661194] [121.49736   31.284826]\n",
      "[121.4990289   31.28548278] [121.49817   31.284775]\n",
      "[121.4963968   31.28344556] [121.500656  31.284937]\n",
      "[121.5015906   31.28543996] [121.49637   31.283644]\n",
      "[121.5006329   31.28617237] [121.496025  31.28456 ]\n",
      "[121.494989    31.28355117] [121.49648   31.284472]\n",
      "[121.4952597   31.28490205] [121.49844   31.285252]\n",
      "[121.4938997   31.28458938] [121.49631   31.284443]\n",
      "[121.5013504   31.28496175] [121.49626   31.284569]\n",
      "[121.5013504   31.28496175] [121.496124  31.284594]\n",
      "[121.4952597   31.28490205] [121.49812  31.28517]\n",
      "[121.4963968   31.28344556] [121.50064   31.284695]\n",
      "[121.4935505   31.28427968] [121.4974    31.284817]\n",
      "[121.4966909   31.28661194] [121.496925  31.284615]\n",
      "[121.4940567   31.28409229] [121.4972    31.284626]\n",
      "[121.5024666   31.28416369] [121.49922   31.283827]\n",
      "[121.4973961  31.285062 ] [121.49646   31.284481]\n",
      "[121.5024638   31.28410231] [121.496445  31.284328]\n",
      "[121.4952597   31.28490205] [121.500015  31.28573 ]\n",
      "[121.5013504   31.28496175] [121.496216  31.284395]\n",
      "[121.497146    31.28451062] [121.49824   31.285069]\n",
      "[121.4960706   31.28294881] [121.49623   31.284481]\n",
      "[121.5009664   31.28562071] [121.50018   31.285595]\n",
      "[121.495492    31.28348255] [121.49769   31.284948]\n",
      "[121.4954682   31.28620791] [121.498024  31.285559]\n",
      "[121.4963968   31.28344556] [121.49624   31.283463]\n",
      "[121.4940567   31.28409229] [121.4989   31.28541]\n",
      "[121.4960706   31.28294881] [121.496216  31.283255]\n",
      "[121.4929114   31.28530074] [121.49596   31.284592]\n",
      "[121.4939771  31.2841402] [121.49626   31.284708]\n",
      "[121.4960706   31.28294881] [121.49626   31.283875]\n",
      "[121.4960706   31.28294881] [121.49617  31.28375]\n",
      "[121.5022152   31.28594734] [121.50032   31.285633]\n",
      "[121.5001968   31.28565588] [121.499054  31.285038]\n",
      "[121.5004486   31.28625488] [121.496185  31.284662]\n",
      "[121.5024477  31.2863657] [121.49974   31.285587]\n",
      "[121.4960706   31.28294881] [121.49619   31.284636]\n",
      "[121.4960706   31.28294881] [121.496376  31.28318 ]\n",
      "[121.4963968   31.28344556] [121.498695  31.28516 ]\n",
      "[121.4938441   31.28458269] [121.496284  31.284418]\n",
      "[121.4971363   31.28450277] [121.4978    31.285225]\n",
      "[121.4960706   31.28294881] [121.49656   31.284466]\n",
      "[121.496059    31.28294492] [121.49651  31.28435]\n",
      "[121.4938832   31.28458576] [121.496284  31.28448 ]\n",
      "[121.4954682   31.28620791] [121.49901   31.285412]\n",
      "[121.4960706   31.28294881] [121.497795  31.283556]\n",
      "[121.4966909   31.28661194] [121.496445  31.284657]\n",
      "[121.4935505   31.28427968] [121.49655   31.284725]\n",
      "[121.4960706   31.28294881] [121.49648   31.284616]\n",
      "[121.4960706   31.28294881] [121.49614   31.282999]\n",
      "[121.4994009   31.28594976] [121.4983    31.283949]\n",
      "[121.5013504   31.28496175] [121.499725  31.285639]\n",
      "[121.4958264   31.28321525] [121.497795  31.285091]\n",
      "[121.4960706   31.28294881] [121.49628   31.283451]\n",
      "[121.4960706   31.28294881] [121.49619  31.28358]\n",
      "[121.4960593   31.28294483] [121.496284  31.283617]\n",
      "[121.4940567   31.28409229] [121.49615   31.283901]\n",
      "[121.4938511   31.28458359] [121.496284  31.284418]\n",
      "[121.5033551   31.28364533] [121.49903   31.285467]\n",
      "[121.4960706   31.28294881] [121.49863   31.285135]\n",
      "[121.4985146   31.28451647] [121.50013  31.28544]\n",
      "[121.4934959   31.28618393] [121.49672  31.2846 ]\n",
      "[121.4963968   31.28344556] [121.49624  31.28332]\n",
      "[121.4932618   31.28435513] [121.49885  31.2852 ]\n",
      "[121.4960706   31.28294881] [121.496574  31.284504]\n",
      "[121.4963968   31.28344556] [121.49637  31.28439]\n",
      "[121.4960706   31.28294881] [121.49742   31.284811]\n",
      "[121.4950601   31.28343436] [121.496414  31.284445]\n",
      "[121.4938321  31.2845783] [121.496216  31.284578]\n",
      "[121.4932096   31.28433647] [121.49616  31.28401]\n",
      "[121.498584    31.28456684] [121.49896  31.28549]\n",
      "[121.5001968   31.28565588] [121.49874   31.284988]\n",
      "[121.5024617   31.28409961] [121.50079   31.284863]\n",
      "[121.4934103   31.28606115] [121.496414  31.284842]\n",
      "[121.4952597   31.28490205] [121.49722  31.28486]\n",
      "[121.4994009   31.28594976] [121.497475  31.28482 ]\n",
      "[121.4929114   31.28530074] [121.49771   31.284851]\n",
      "[121.4940567   31.28409229] [121.4967    31.284811]\n",
      "[121.5018883   31.28604362] [121.496376  31.284237]\n",
      "[121.4994009   31.28594976] [121.49631  31.28466]\n",
      "[121.4973961  31.285062 ] [121.49664   31.284443]\n",
      "[121.4937943   31.28467898] [121.49621   31.284605]\n",
      "[121.4957139   31.28355654] [121.4981    31.285353]\n",
      "[121.4960706   31.28294881] [121.49632   31.283548]\n",
      "[121.4937942   31.28467896] [121.49614   31.284576]\n",
      "[121.4960706   31.28294881] [121.49639   31.283335]\n",
      "[121.4938224   31.28464827] [121.49616  31.2846 ]\n",
      "[121.4937947   31.28467975] [121.49626  31.28457]\n",
      "[121.5024607   31.28409658] [121.500656  31.2845  ]\n",
      "[121.4960706   31.28294881] [121.49614   31.284086]\n",
      "[121.4935505   31.28427968] [121.49952   31.285505]\n",
      "[121.5021622   31.28611677] [121.50052   31.284882]\n",
      "[121.496297    31.28354031] [121.49617  31.28303]\n",
      "[121.5026421   31.28468929] [121.50177   31.284378]\n",
      "[121.499012    31.28543846] [121.498215  31.284773]\n",
      "[121.4960706   31.28294881] [121.49626   31.283829]\n",
      "[121.4973388   31.28469532] [121.49791   31.285124]\n",
      "[121.4960706   31.28294881] [121.496124  31.283274]\n",
      "[121.4960706   31.28294881] [121.497635  31.284801]\n",
      "[121.4960706   31.28294881] [121.49683   31.284498]\n",
      "[121.495481    31.28365498] [121.497986  31.285023]\n",
      "[121.4954589  31.2835592] [121.49743   31.284632]\n",
      "[121.4960706   31.28294881] [121.49612   31.282887]\n",
      "[121.4960706   31.28294881] [121.498436  31.285257]\n",
      "[121.4973961  31.285062 ] [121.4963    31.283436]\n",
      "[121.4968664   31.28650678] [121.49864   31.285105]\n",
      "[121.495768    31.28305968] [121.49667   31.284637]\n",
      "[121.5021748   31.28611176] [121.498604  31.285326]\n",
      "[121.4960706   31.28294881] [121.49612   31.283203]\n",
      "[121.498661   31.2845859] [121.497314  31.285082]\n",
      "[121.500988    31.28571026] [121.49942   31.285381]\n",
      "[121.4952159   31.28496042] [121.496735  31.284386]\n",
      "[121.4990673   31.28449594] [121.49676   31.284521]\n",
      "[121.4960596   31.28294508] [121.49642   31.284353]\n",
      "[121.4962305   31.28330383] [121.49628  31.28338]\n",
      "[121.4960706   31.28294881] [121.496284  31.283398]\n",
      "[121.4935505   31.28427968] [121.49676  31.28457]\n",
      "[121.4954682   31.28620791] [121.4997    31.285643]\n",
      "[121.4938997   31.28458938] [121.496284  31.28448 ]\n",
      "[121.4925859   31.28436426] [121.49667   31.284563]\n",
      "[121.4943037   31.28666155] [121.496216  31.284239]\n",
      "[121.4966909   31.28661194] [121.49679   31.285635]\n",
      "[121.5018891   31.28604664] [121.49607   31.284082]\n",
      "[121.496297    31.28354031] [121.49632   31.283575]\n",
      "[121.4932927   31.28439685] [121.49808   31.285069]\n",
      "[121.4952597   31.28490205] [121.499214  31.2854  ]\n",
      "[121.4925513   31.28432898] [121.497055  31.284628]\n",
      "[121.4958189   31.28316484] [121.500725  31.2851  ]\n",
      "[121.5033977  31.2836985] [121.498375  31.285275]\n",
      "[121.4963968   31.28344556] [121.4969    31.285604]\n",
      "[121.4938392  31.2846033] [121.49628   31.284649]\n",
      "[121.4960706   31.28294881] [121.49617  31.28369]\n",
      "[121.4934969   31.28617289] [121.49672  31.2846 ]\n",
      "[121.4960706   31.28294881] [121.49762  31.28502]\n",
      "[121.4960706   31.28294881] [121.497055  31.28453 ]\n",
      "[121.5014108   31.28523057] [121.4964    31.283958]\n",
      "[121.4966909   31.28661194] [121.496994  31.284756]\n",
      "[121.5006329   31.28617237] [121.49824   31.285069]\n",
      "[121.4960763   31.28295643] [121.4961    31.284557]\n",
      "[121.4958227   31.28315556] [121.4975    31.285307]\n",
      "[121.4952159   31.28496104] [121.49676   31.284807]\n",
      "[121.4992341   31.28598299] [121.49621   31.284618]\n",
      "[121.4935505   31.28427968] [121.49769   31.284895]\n",
      "[121.5024865   31.28414226] [121.49616   31.284155]\n",
      "[121.4963968   31.28344556] [121.49751   31.284742]\n",
      "[121.4994009   31.28594976] [121.49658   31.284428]\n",
      "[121.4960706   31.28294881] [121.49619   31.284088]\n",
      "[121.4973961  31.285062 ] [121.49607   31.284552]\n",
      "[121.503607    31.28368829] [121.49771   31.285097]\n",
      "[121.5016544   31.28561818] [121.49805  31.28496]\n",
      "[121.4990212   31.28520186] [121.4986    31.284868]\n",
      "[121.4952597   31.28490205] [121.499214  31.2854  ]\n",
      "[121.5013504   31.28496175] [121.49954  31.28544]\n",
      "[121.4953794   31.28368547] [121.49814   31.284992]\n",
      "[121.4960706   31.28294881] [121.496056  31.283031]\n",
      "[121.5031177   31.28376034] [121.499626  31.28394 ]\n",
      "[121.4974254  31.2864051] [121.49952  31.2855 ]\n",
      "[121.4960706   31.28294881] [121.49663   31.284718]\n",
      "[121.5016567   31.28562006] [121.49754   31.284801]\n",
      "[121.4994009   31.28594976] [121.49796   31.284222]\n",
      "[121.4963968   31.28344556] [121.49624   31.283463]\n",
      "[121.5019866   31.28608967] [121.500885  31.284693]\n",
      "[121.4954764   31.28352778] [121.49832   31.285143]\n",
      "[121.5013504   31.28496175] [121.49631   31.284332]\n",
      "[121.4954682   31.28620791] [121.49628  31.28386]\n",
      "[121.4973961  31.285062 ] [121.49655   31.284472]\n",
      "[121.4960706   31.28294881] [121.496124  31.283262]\n",
      "[121.4994009   31.28594976] [121.49619   31.284578]\n",
      "[121.4929114   31.28530074] [121.49699   31.284843]\n",
      "[121.5017952   31.28609274] [121.49651   31.283997]\n",
      "[121.4963968   31.28344556] [121.501396  31.28571 ]\n",
      "[121.4952158   31.28496091] [121.49658   31.284702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.4960706   31.28294881] [121.49674  31.28448]\n",
      "[121.50109     31.28575199] [121.50277   31.286121]\n",
      "[121.4960597   31.28294521] [121.4961    31.284613]\n",
      "[121.5023887   31.28404393] [121.50069  31.28482]\n",
      "[121.4960706   31.28294881] [121.496185  31.283152]\n",
      "[121.4960706   31.28294881] [121.49614   31.283228]\n",
      "[121.4963968   31.28344556] [121.49789   31.284843]\n",
      "[121.4952597   31.28490205] [121.49974   31.284046]\n",
      "[121.4963968   31.28344556] [121.49621   31.283588]\n",
      "[121.4935075   31.28621248] [121.496346  31.284021]\n",
      "[121.4960706   31.28294881] [121.49708  31.28482]\n",
      "[121.4960706   31.28294881] [121.49812   31.285006]\n",
      "[121.4960706   31.28294881] [121.49619   31.283028]\n",
      "[121.4952597   31.28490205] [121.4994    31.283869]\n",
      "[121.4958333   31.28314025] [121.4975    31.285307]\n",
      "[121.5013776  31.2860664] [121.496346  31.283785]\n",
      "[121.4960606   31.28294557] [121.49676   31.284483]\n",
      "[121.4954762   31.28365455] [121.49846   31.285334]\n",
      "[121.5005501   31.28639365] [121.497665  31.284916]\n",
      "[121.4960706   31.28294881] [121.498     31.285116]\n",
      "[121.4973709   31.28641534] [121.49865   31.285223]\n",
      "[121.5005665   31.28610338] [121.49798   31.285164]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_new)):\n",
    "    print(y_valid_pd.values[i], y_new[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
