{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 数据预处理\n",
    "path = r\"./siping_4g-2.csv\"\n",
    "\n",
    "\n",
    "# 经纬度数据归一化处理\n",
    "def preprocess(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.fillna(-1)  # 缺失值全部为-1\n",
    "    yscaler = MinMaxScaler()\n",
    "    yscaler.fit(data[['Longitude', 'Latitude']].values)\n",
    "\n",
    "    return yscaler\n",
    "\n",
    "yscaler = preprocess(path)\n",
    "# print(yscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建时间序列\n",
    "\n",
    "MRTime = [\n",
    "1543994925358,\n",
    "1543994928431,\n",
    "1543994931521,\n",
    "1543994934582,\n",
    "1543994937645]\n",
    "\n",
    "import datetime\n",
    "\n",
    "# 计算时间差 以秒为单位\n",
    "def compute_time_interval(timestamp1, timestamp2):\n",
    "    dateArray1 = datetime.datetime.utcfromtimestamp(timestamp1 / 1000)\n",
    "    dateArray2 = datetime.datetime.utcfromtimestamp(timestamp2 / 1000)\n",
    "    # otherStyleTime = dateArray1.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return float((dateArray2 - dateArray1).seconds)\n",
    "\n",
    "ti = np.zeros((len(MRTime)))\n",
    "for j in range(len(MRTime)):\n",
    "    if j == 0:\n",
    "        ti[j] = 0\n",
    "    else:\n",
    "        ti[j] = compute_time_interval(MRTime[j - 1], MRTime[j])\n",
    "print(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 计算时间差 以秒为单位\n",
    "def compute_time_interval(timestamp1, timestamp2):\n",
    "    dateArray1 = datetime.datetime.utcfromtimestamp(timestamp1 / 1000)\n",
    "    dateArray2 = datetime.datetime.utcfromtimestamp(timestamp2 / 1000)\n",
    "    # otherStyleTime = dateArray1.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return float((dateArray2 - dateArray1).seconds)\n",
    "\n",
    "\n",
    "# 自变量字段信息\n",
    "feature_col = ['RNCID_1', 'RNCID_2', 'RNCID_3', 'RNCID_4', 'RNCID_5', 'RNCID_6', 'RNCID_7',\n",
    "               'CellID_1', 'CellID_2', 'CellID_3', 'CellID_4', 'CellID_5', 'CellID_6', 'CellID_7',\n",
    "               'Dbm_1', 'Dbm_2', 'Dbm_3', 'Dbm_4', 'Dbm_5', 'Dbm_6', 'Dbm_7',\n",
    "               'AsuLevel_1', 'AsuLevel_2', 'AsuLevel_3', 'AsuLevel_4', 'AsuLevel_5', 'AsuLevel_6', 'AsuLevel_7',\n",
    "               'SignalLevel_1', 'SignalLevel_2', 'SignalLevel_3', 'SignalLevel_4', 'SignalLevel_5', 'SignalLevel_6', 'SignalLevel_7']\n",
    "\n",
    "\n",
    "# 时间序列的最大长度\n",
    "Max_S = 10\n",
    "\n",
    "# 一个MR数据表征为一个7*5矩阵\n",
    "bs_c = 7\n",
    "col = 5\n",
    "\n",
    "\n",
    "# 构造序列数据\n",
    "def make_sequence1(data, yscaler, max_subs=Max_S):\n",
    "    seq_list = []\n",
    "    time_list = []\n",
    "    loc_list = []\n",
    "    mode_list = []\n",
    "    # 根据Trajec ID 进行分组  --- 这是一段轨迹\n",
    "    trajs = data.groupby([\"TrajID\"])\n",
    "    max_seq = 0\n",
    "    max_subseq = 0\n",
    "    for trajid, traj in trajs:\n",
    "        # 获取一段轨迹的信息\n",
    "        dt = traj[feature_col].values\n",
    "        # dt = xscaler.transform(traj[feature_col].values)\n",
    "\n",
    "        # 将经纬度进行归一化\n",
    "        lc = yscaler.transform(traj[['Longitude', 'Latitude']].values)\n",
    "        tl = traj['MRTime'] # 记录MR的时间\n",
    "        md = traj['mode']   # 数据的mode  run walk ...\n",
    "        # traj = traj.sort_values([\"MRTime\"])\n",
    "        _ = traj.shape[0] % max_subs\n",
    "        c = int(traj.shape[0] / max_subs)\n",
    "\n",
    "        if traj.shape[0] <= max_subs:\n",
    "            seq = np.zeros((max_subs, bs_c * col))  # 序列数据结构： 10 * （7*5）\n",
    "            loc = np.ones((max_subs, 2)) * (-1)     # 经纬度数据    10 * 2 \n",
    "            # ti 时间戳\n",
    "            t_tl, m_l, ti = np.zeros((max_subs)), np.zeros((max_subs)), np.zeros((max_subs))\n",
    "            seq[0: traj.shape[0], :] = dt[0: traj.shape[0], :]      # 自变量序列\n",
    "            loc[0: traj.shape[0], :] = lc[0: traj.shape[0], :]      # 经纬度\n",
    "            t_tl[0: traj.shape[0]] = np.array(tl[0: traj.shape[0]]) # time序列\n",
    "            m_l[0: traj.shape[0]] = md[0: traj.shape[0]]  # mode\n",
    "            for j in range(traj.shape[0]):\n",
    "                if j == 0:\n",
    "                    ti[j] = 0\n",
    "                else:\n",
    "                    ti[j] = compute_time_interval(t_tl[j - 1], t_tl[j])\n",
    "            seq_list.append(seq)   # 时间序列\n",
    "            loc_list.append(loc)   # \n",
    "            time_list.append(ti)   # 时间戳序列\n",
    "            mode_list.append(m_l)\n",
    "        else:\n",
    "            start = 0\n",
    "            while start <= traj.shape[0] - max_subs - 1:\n",
    "                seq = np.zeros((max_subs, bs_c * col))\n",
    "                loc = np.ones((max_subs, 2)) * (-1)\n",
    "                t_tl, m_l, ti = np.zeros((max_subs)), np.zeros((max_subs)), np.zeros((max_subs))\n",
    "                #                 print (start, start+max_sub, seq[start: start + max_subs, :].shape, dt[start: start + max_subs, :].shape)\n",
    "                seq = dt[start: start + max_subs, :]\n",
    "                loc = lc[start: start + max_subs, :]\n",
    "                t_tl = np.array(tl[start: start + max_subs])\n",
    "                m_l = md[start: start + max_subs]\n",
    "                #                 t_tl = np.array()\n",
    "                for j in range(max_subs):\n",
    "                    if j == 0:\n",
    "                        ti[j] = 0\n",
    "                    else:\n",
    "                        ti[j] = compute_time_interval(t_tl[j - 1], t_tl[j])\n",
    "                seq_list.append(seq)\n",
    "                loc_list.append(loc)\n",
    "                time_list.append(ti)\n",
    "                mode_list.append(m_l)\n",
    "                start += max_subs\n",
    "\n",
    "            if start != traj.shape[0] - 1:\n",
    "                seq = np.zeros((max_subs, bs_c * col))\n",
    "                loc = np.ones((max_subs, 2)) * (-1)\n",
    "                t_tl, m_l, ti = np.zeros((max_subs)), np.zeros((max_subs)), np.zeros((max_subs))\n",
    "                seq[0: traj.shape[0] - start, :] = dt[start: traj.shape[0], :]\n",
    "                loc[0: traj.shape[0] - start, :] = lc[start: traj.shape[0], :]\n",
    "                t_tl[0: traj.shape[0] - start] = np.array(tl[start: traj.shape[0]])\n",
    "                m_l[0: traj.shape[0] - start] = md[start: traj.shape[0]]\n",
    "                for j in range(max_subs):\n",
    "                    if j == 0:\n",
    "                        ti[j] = 0\n",
    "                    else:\n",
    "                        ti[j] = compute_time_interval(t_tl[j - 1], t_tl[j])\n",
    "                seq_list.append(seq)\n",
    "                loc_list.append(loc)\n",
    "                time_list.append(ti)\n",
    "                mode_list.append(m_l)\n",
    "\n",
    "    return seq_list, loc_list, time_list, mode_list\n",
    "\n",
    "\n",
    "# 准备序列数据\n",
    "def seq_sli(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.fillna(-1)  # -1填充\n",
    "    a, b, c, d = [], [], [], []\n",
    "    ea, eb, ec, ed = [], [], [], []\n",
    "    te = pd.DataFrame()\n",
    "    for i in range(5):\n",
    "        tmp = data.loc[range(i, len(data), 5), :]\n",
    "        if i < 4:\n",
    "            ta, tb, tc, td = make_sequence1(tmp, yscaler)\n",
    "        else:\n",
    "            ta, tb, tc, td = make_sequence1(tmp, yscaler)\n",
    "        if i == 0:\n",
    "            a, b, c, d = ta, tb, tc, td\n",
    "        elif i < 4:\n",
    "            a.extend(ta)\n",
    "            b.extend(tb)\n",
    "            c.extend(tc)\n",
    "            d.extend(td)\n",
    "        else:\n",
    "            ea, eb, ec, ed = ta, tb, tc, td\n",
    "\n",
    "    train_dataset = np.array(a)\n",
    "    train_label = np.array(b)\n",
    "    train_time = np.array(c)\n",
    "    train_mode = np.array(d)\n",
    "\n",
    "    test_dataset = np.array(ea)\n",
    "    test_label = np.array(eb)\n",
    "    test_time = np.array(ec)\n",
    "    test_mode = np.array(ed)\n",
    "\n",
    "    return train_dataset, train_label, train_time, train_mode, test_dataset, test_label, test_time, test_mode\n",
    "\n",
    "# 获取数据\n",
    "tr_d, tr_l, tr_t, tr_m, te_d, te_l, te_t, te_m = seq_sli(path)\n",
    "\n",
    "print(tr_d.shape)\n",
    "print(tr_l.shape)\n",
    "print(tr_t.shape)\n",
    "print(tr_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 准备数据: 讲数据形状变为模型需要的形状\n",
    "BATCH_S = 4   # batch_size\n",
    "\"\"\"\n",
    "fea: train_data （特征值）\n",
    "lab: train_label（经纬度）\n",
    "tl: time_list （时间序列）\n",
    "md: mode list\n",
    "\"\"\"\n",
    "\n",
    "def data_prepare(fea, lab, tl, md):\n",
    "    batch_size = BATCH_S\n",
    "    f, l, t, d = [], [], [], []\n",
    "    _ = fea.shape[0] % batch_size\n",
    "    batch_cnt = int(fea.shape[0] / batch_size)\n",
    "    if _ != 0:\n",
    "        batch_cnt += 1\n",
    "    for i in range(0, batch_cnt):\n",
    "        tmp_f = np.zeros((batch_size * Max_S, bs_c, col, 1))\n",
    "        tmp_l = np.ones((batch_size * Max_S, 2)) * (-1)\n",
    "        tmp_t = np.ones((batch_size * Max_S)) * (-1)\n",
    "        tmp_m = np.ones((batch_size * Max_S)) * (-1)\n",
    "        if i < batch_cnt - 1:\n",
    "            for j in range(0, batch_size):\n",
    "                f_t = fea[i * batch_size + j, :, :].reshape([Max_S, bs_c, col, 1])\n",
    "                l_t = lab[i * batch_size + j, :]\n",
    "                t_t = tl[i * batch_size + j]\n",
    "                m_t = md[i * batch_size + j]\n",
    "                for m in range(0, Max_S):\n",
    "                    #                     print (j, m, j*Max_S+m)\n",
    "                    tmp_f[j * Max_S + m, :, :, :] = f_t[m, :, :, :]\n",
    "                    tmp_l[j * Max_S + m, :] = l_t[m, :]\n",
    "                    tmp_t[j * Max_S + m] = t_t[m]\n",
    "                    tmp_m[j * Max_S + m] = m_t[m]\n",
    "        else:\n",
    "            if _ == 0:\n",
    "                for j in range(0, batch_size):\n",
    "                    f_t = fea[i * batch_size + j, :, :].reshape([Max_S, bs_c, col, 1])\n",
    "                    l_t = lab[i * batch_size + j, :]\n",
    "                    t_t = tl[i * batch_size + j]\n",
    "                    m_t = md[i * batch_size + j]\n",
    "                    for m in range(0, Max_S):\n",
    "                        tmp_f[j * Max_S + m, :, :, :] = f_t[m, :, :, :]\n",
    "                        tmp_l[j * Max_S + m, :] = l_t[m, :]\n",
    "                        tmp_t[j * Max_S + m] = t_t[m]\n",
    "                        tmp_m[j * Max_S + m] = m_t[m]\n",
    "            else:\n",
    "                for j in range(0, _):\n",
    "                    f_t = fea[i * batch_size + j, :, :].reshape([Max_S, bs_c, col, 1])\n",
    "                    l_t = lab[i * batch_size + j, :]\n",
    "                    t_t = tl[i * batch_size + j]\n",
    "                    m_t = md[i * batch_size + j]\n",
    "                    for m in range(0, Max_S):\n",
    "                        tmp_f[j * Max_S + m, :, :, :] = f_t[m, :, :, :]\n",
    "                        tmp_l[j * Max_S + m, :] = l_t[m, :]\n",
    "                        tmp_t[j * Max_S + m] = t_t[m]\n",
    "                        tmp_m[j * Max_S + m] = m_t[m]\n",
    "\n",
    "                for j in range(_, batch_size):\n",
    "                    f_t = fea[(i - 1) * batch_size + (_ - 1), :, :].reshape([Max_S, bs_c, col, 1])\n",
    "                    l_t = lab[(i - 1) * batch_size + (_ - 1), :]\n",
    "                    t_t = tl[(i - 1) * batch_size + (_ - 1)]\n",
    "                    m_t = md[(i - 1) * batch_size + (_ - 1)]\n",
    "                    for m in range(0, Max_S):\n",
    "                        tmp_f[j * Max_S + m, :, :, :] = f_t[m, :, :, :]\n",
    "                        tmp_l[j * Max_S + m, :] = l_t[m, :]\n",
    "                        tmp_t[j * Max_S + m] = t_t[m]\n",
    "                        tmp_m[j * Max_S + m] = m_t[m]\n",
    "\n",
    "        f.append(tmp_f)\n",
    "        l.append(tmp_l)\n",
    "        t.append(tmp_t)\n",
    "        d.append(tmp_m)\n",
    "\n",
    "    return f, l, t, d\n",
    "\n",
    "\n",
    "def re_npy(path):\n",
    "    tr_d, tr_l, tr_t, tr_m, te_d, te_l, te_t, te_m = seq_sli(path)\n",
    "    # 构建训练数据集  形状\n",
    "    f, l, t, m = data_prepare(tr_d, tr_l, tr_t, tr_m)\n",
    "    f = np.array(f)\n",
    "    l = np.array(l)\n",
    "    t = np.array(t)\n",
    "    m = np.array(m)\n",
    "    # 构建测试数据集 形状\n",
    "    ef, el, et, em = data_prepare(te_d, te_l, te_t, te_m)\n",
    "    ef = np.array(ef)\n",
    "    el = np.array(el)\n",
    "    et = np.array(et)\n",
    "    em = np.array(em)\n",
    "\n",
    "    return f, l, m, t, ef, el, em, et\n",
    "\n",
    "f, l, m, t, ef, el, em, et = re_npy(path)\n",
    "\n",
    "print(f.shape)\n",
    "print(l.shape)\n",
    "print(m.shape)\n",
    "print(t.shape)\n",
    "print(ef.shape)\n",
    "print(el.shape)\n",
    "print(em.shape)\n",
    "print(et.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算 m_hist\n",
    "import math\n",
    "\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0\n",
    "\n",
    "# 计算真假误差\n",
    "def distance(true_pt, pred_pt):\n",
    "    lat1 = float(true_pt[1])\n",
    "    lng1 = float(true_pt[0])\n",
    "    lat2 = float(pred_pt[1])\n",
    "    lng2 = float(pred_pt[0])\n",
    "    radLat1 = rad(lat1)\n",
    "    radLat2 = rad(lat2)\n",
    "    a = radLat1 - radLat2\n",
    "    b = rad(lng1) - rad(lng2)\n",
    "    s = 2 * math.asin(math.sqrt(math.pow(math.sin(a / 2), 2) +\n",
    "                                math.cos(radLat1) * math.cos(radLat2) * math.pow(math.sin(b / 2), 2)))\n",
    "    s = s * 6378.137\n",
    "    s = round(s * 10000) / 10\n",
    "    return s\n",
    "\n",
    "def mode_dict(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.fillna(-1)\n",
    "    trajs = data.groupby([\"TrajID\"])\n",
    "    m_d = {}\n",
    "    m_d[0] = []\n",
    "    m_d[1] = []\n",
    "    m_d[2] = []\n",
    "    for trajid, traj in trajs:\n",
    "        md = list(traj['mode'])\n",
    "        loc = traj[['Longitude', 'Latitude']].values\n",
    "        tl = list(traj['MRTime'])\n",
    "        for j in range(1, traj.shape[0]):\n",
    "            if j > 0:\n",
    "                delta_t = compute_time_interval(tl[j - 1], tl[j])\n",
    "                delta_s = distance(loc[j - 1, :], loc[j, :])\n",
    "                m = int(md[j])\n",
    "                if delta_t > 0:\n",
    "                    m_d[m].append(delta_s / delta_t)\n",
    "    return m_d\n",
    "\n",
    "def speed_hist(mode_dict):\n",
    "    mode_hist_dict = {}\n",
    "    for key, value in mode_dict.items():\n",
    "        tmp = np.zeros((7))\n",
    "        for t in value:\n",
    "            if t <= 2:\n",
    "                tmp[0] += 1\n",
    "            elif t <= 6:\n",
    "                tmp[1] += 1\n",
    "            elif t <= 12:\n",
    "                tmp[2] += 1\n",
    "            elif t <= 18:\n",
    "                tmp[3] += 1\n",
    "            elif t <= 24:\n",
    "                tmp[4] += 1\n",
    "            elif t <= 30:\n",
    "                tmp[5] += 1\n",
    "            else:\n",
    "                tmp[6] += 1\n",
    "        prob = []\n",
    "        if len(value) > 0:\n",
    "            for t in tmp:\n",
    "                prob.append(float(t) / float(len(value)))\n",
    "        mode_hist_dict[key] = prob\n",
    "\n",
    "    return mode_hist_dict\n",
    "\n",
    "_ = mode_dict(path)\n",
    "# print(_)\n",
    "m_hist = speed_hist(_)\n",
    "print(m_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入层\n",
    "model.add(Convolution2D(\n",
    "        input_shape=(7, 4, 1),# 输入形状 \n",
    "          filters=64,         # 卷积中滤波器的输出数量\n",
    "          kernel_size=3,      # 卷积核\n",
    "          strides=1,          # 步长\n",
    "          padding='same',\n",
    "          activation='relu')) # 激活函数\n",
    "\n",
    "# 输出层\n",
    "model.add(Dense(2, activation='lineaer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入层\n",
    "tf.keras.layers.Conv2D(filters=64,        # 卷积中滤波器的输出数量\n",
    "                       kernel_size=(1, 5),# 卷积核\n",
    "                       padding='same',   # 多余的列舍弃\n",
    "                       strides=(1, 1),    # 步长\n",
    "                       input_shape=(7, 5, 1),# 输入形状 \n",
    "                       activation='relu')    # 激活函数\n",
    "\n",
    "# 输出层\n",
    "tf.keras.layers.Dense(2, activation='lineaer')\n",
    "\n",
    "tf.keras.layers.Dense(2, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "\n",
    "mode_class_num = 3\n",
    "\n",
    "\n",
    "class Local(tf.keras.layers.Layer):\n",
    "    def __init__(self, latent_dim=64, intermediate_dim=128, name='Local', **kwargs):\n",
    "        super(Local, self).__init__(name=name, **kwargs)\n",
    "        self.cnn = tf.keras.layers.Conv2D(filters=64, kernel_size=(1, col), padding='valid',\n",
    "                                          input_shape=(7, col, 1), activation='relu', name='localcnn')\n",
    "        self.bn = tf.keras.layers.BatchNormalization(name='localbn')  # 把一个batch内的所有数据，从不规范的分布拉到正态分布\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2, name='localdrop') # 防止过拟合，随机0.2的不进行工作\n",
    "        self.reshape = tf.keras.layers.Reshape((7, 64), name='localreshape')  # 任意层之间连接\n",
    "        self.lstm = tf.keras.layers.LSTM(128, return_sequences=False, stateful=False, name='locallstm') # 输出维度？\n",
    "        self.dense_1 = tf.keras.layers.Dense(intermediate_dim, activation='relu', name='localdense1') # 输出维度128\n",
    "        self.dense_2 = tf.keras.layers.Dense(latent_dim, activation='relu', name='localdense2')       # 输出维度 64\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cnn_bn = self.bn(self.cnn(inputs))\n",
    "        cnn_drop = self.dropout(cnn_bn)\n",
    "        lstm_inp = self.reshape(cnn_drop)\n",
    "        # print (lstm_inp.shape)\n",
    "        lstm_out = self.lstm(lstm_inp)\n",
    "        lstm_drop = self.dropout(lstm_out)\n",
    "        dense_1 = self.dropout(self.dense_1(lstm_drop))\n",
    "        local_out = self.dense_2(dense_1)   # 64维度的数据\n",
    "        return local_out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Local, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_size=32, name='Attention', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.attention_size = attention_size    # 输入32维\n",
    "        self.dense_1 = tf.keras.layers.Dense(attention_size, activation='tanh', name='attndnese1')  # 全连接 32\n",
    "        self.dense_2 = tf.keras.layers.Dense(1, use_bias=False, name='attndense2')   # 全连接 1维\n",
    "\n",
    "    def call(self, inputs):\n",
    "        v = self.dense_1(inputs)\n",
    "        vu = self.dense_2(v)\n",
    "        alphas = tf.nn.softmax(vu)   # 一维数据经过 softmax  映射到0和1之间\n",
    "        output = alphas\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Attention, self).get_config()\n",
    "        config.update({\"attention_size\": self.attention_size})\n",
    "        return config\n",
    "\n",
    "\n",
    "import AttentionCell\n",
    "from imp import reload\n",
    "\n",
    "reload(AttentionCell)\n",
    "\n",
    "class Global(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_subs=Max_S, latent_dim=64, name='Global', **kwargs):\n",
    "        super(Global, self).__init__(name=name, **kwargs)\n",
    "        self.max_subs = max_subs\n",
    "\n",
    "        self.cell = AttentionCell.ALSTM_Cell(64, name='attncell')  # 注意力机制：输入64维度\n",
    "        self.rnn = tf.keras.layers.RNN(self.cell, return_sequences=True, name='attnRNN')  # RNN中使用Attention Cell\n",
    "        self.dense_1 = tf.keras.layers.Dense(64, activation='relu', \n",
    "                                             name='globaldense1')  # tf.keras.layers.BatchNormalization(), 全连接 64层\n",
    "        self.dense_2 = tf.keras.layers.Dense(32, activation='relu', name='globaldense2')   # 全连接 32 层\n",
    "        self.out = tf.keras.layers.Dense(2, activation='sigmoid', name='globalout')        # 全连接 2 层\n",
    "        self.attn = Attention(name='globalattn')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        # mask = self.mask(x)\n",
    "        h = self.rnn(x)   # 接受64维度数据\n",
    "        h_d_1 = self.dense_1(h)      # 64\n",
    "        h_d_2 = self.dense_2(h_d_1)  # 32\n",
    "        h_a = self.attn(h_d_2)       # 使用Attention  返回结果为一维数据，映射在0-1之间\n",
    "        y = self.out(h_d_2 * h_a)    # 输出2维数据     32维数据与0-1之间一个数据相乘，然后映射为2维数据\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Global, self).get_config()\n",
    "        config.update({\"max_subs\": self.max_subs})\n",
    "        return config\n",
    "\n",
    "class Seq(tf.keras.Model):\n",
    "    def __init__(self, latent_dim=64, intermediate_dim=128, name='Seq', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.local_out_dim = latent_dim\n",
    "        self.local = Local(latent_dim=latent_dim, intermediate_dim=intermediate_dim, name='seqlocal')   # 输出64 维度的数据\n",
    "        self.pred = Global(name='seqglobal')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_inputs = inputs[0]\n",
    "        batch_time = inputs[1]\n",
    "\n",
    "        x = self.local(batch_inputs)\n",
    "        x = tf.reshape(x, [BATCH_S, Max_S, self.local_out_dim])\n",
    "        batch_time = tf.reshape(batch_time, [BATCH_S, Max_S, 1])\n",
    "        x = tf.keras.layers.concatenate([x, batch_time], axis=-1)\n",
    "        x = self.pred(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Seq, self).get_config()\n",
    "        config.update({\"local_out_dim\": self.local_out_dim})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_model = Seq()\n",
    "\n",
    "# print(raw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 自定义评判指标\n",
    "def proba(v, mode, m_hist):\n",
    "    prob = 0\n",
    "    if mode >= 0:\n",
    "        ref = m_hist[mode]\n",
    "        if v <= 2:\n",
    "            prob = ref[0]\n",
    "        elif v <= 6:\n",
    "            prob = ref[1]\n",
    "        elif v <= 12:\n",
    "            prob = ref[2]\n",
    "        elif v <= 18:\n",
    "            prob = ref[3]\n",
    "        elif v <= 24:\n",
    "            prob = ref[4]\n",
    "        elif v <= 30:\n",
    "            prob = ref[5]\n",
    "        else:\n",
    "            prob = ref[6]\n",
    "    return prob\n",
    "\n",
    "def mode_const(y_pred, time_list, mode_list, yscaler, m_hist):\n",
    "    sum_p = 0\n",
    "    c = 0\n",
    "    for i in range(1, y_pred.shape[0]):\n",
    "        mode = int(mode_list[i].numpy())\n",
    "        if mode >= 0:\n",
    "            loc_former = yscaler.inverse_transform(y_pred[i - 1, :].reshape(1, 2))\n",
    "            loc_later = yscaler.inverse_transform(y_pred[i, :].reshape(1, 2))\n",
    "            dis = distance(loc_former[0], loc_later[0])\n",
    "            v = dis / time_list[i]\n",
    "            p = - math.log(1 + proba(v, mode, m_hist))\n",
    "            sum_p += p\n",
    "            c += 1\n",
    "    if c > 0:\n",
    "        return sum_p / c\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def loss_function_const(y_true, y_pred, time_list, mode_list, yscaler, m_hist):\n",
    "    judge = tf.math.equal(y_true, -1.0)  # 经纬度不能等于 -1 [True, False]\n",
    "    mask = tf.math.logical_not(tf.math.logical_and(judge[:, 0], judge[:, 1]))\n",
    "\n",
    "    loss_ = tf.keras.losses.MSE(y_true, y_pred)\n",
    "    loss_ = tf.cast(loss_, dtype='float32')\n",
    "    count = tf.math.count_nonzero(mask)\n",
    "    count = tf.cast(count, dtype='float32')\n",
    "    mask = tf.cast(mask, dtype='float32')\n",
    "    loss_ *= mask\n",
    "\n",
    "    constraint = mode_const(y_pred.numpy(), time_list, mode_list, yscaler, m_hist)\n",
    "    thres_alpha = 0.05\n",
    "\n",
    "    return tf.reduce_sum(loss_) / (count + 1) + thres_alpha * constraint\n",
    "\n",
    "\n",
    "def predict_t(model, fea, lab, time, lox_sel, batch_size=BATCH_S):\n",
    "    pred = []\n",
    "    true = []\n",
    "    for i in range(0, fea.shape[0]):\n",
    "        x_batch = fea[i, :, :, :, :]\n",
    "        y_batch = lab[i, :, :].reshape((batch_size * Max_S, 2))\n",
    "        t_batch = time[i, :]\n",
    "        x_batch = tf.cast(x_batch, tf.float32)\n",
    "        t_batch = tf.cast(t_batch, tf.float32)\n",
    "        batch_input = []\n",
    "        batch_input.append(x_batch)\n",
    "        batch_input.append(t_batch)\n",
    "\n",
    "        #         pred_batch = model(x_batch, t_batch)\n",
    "        if lox_sel == 0:\n",
    "            pred_batch = model(batch_input)\n",
    "        else:\n",
    "            pred_batch, _ = model(batch_input)\n",
    "        pred_batch = tf.reshape(pred_batch, [batch_size * Max_S, 2]).numpy()\n",
    "        if i == 0:\n",
    "            pred = pred_batch\n",
    "            true = y_batch\n",
    "        else:\n",
    "            pred = np.concatenate((pred, pred_batch), axis=0) # 拼接\n",
    "            true = np.concatenate((true, y_batch), axis=0)\n",
    "    return pred, true\n",
    "\n",
    "def f_error(tr_loc, Y, yscaler):\n",
    "    err = []\n",
    "    m = 0\n",
    "\n",
    "    while m < tr_loc.shape[0]:\n",
    "        if Y[m, 0] != -1.0 and Y[m, 1] != -1.0:\n",
    "            tr_loc_real = yscaler.inverse_transform(tr_loc[m, :].reshape(1, 2))\n",
    "            y_true_real = yscaler.inverse_transform(Y[m, :].reshape(1, 2))\n",
    "            err.append(distance(tr_loc_real.reshape(2), y_true_real.reshape(2)))\n",
    "        m += 1\n",
    "\n",
    "    # print(m)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_batch_train = f[0, :, :, :, :]\n",
    "y_batch_train = l[0, :, :]\n",
    "t_batch_time = t[0, :]\n",
    "m_batch_time = m[0, :]\n",
    "\n",
    "print(type(x_batch_train))\n",
    "print(x_batch_train.shape)  # samples\n",
    "print(y_batch_train.shape)  # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Max_S = 10\n",
    "lox_sel = 0\n",
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "# 损失函数\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "m_med_err = 999\n",
    "m_mea_err = 999\n",
    "m_nin_err = 999\n",
    "max_acc = 0\n",
    "\n",
    "shuffle_list = random.sample(range(f.shape[0]), f.shape[0])\n",
    "\n",
    "yscaler = preprocess(path)\n",
    "# print(\"yscaler: \", yscaler.feature_range)\n",
    "\n",
    "# 训练轮数\n",
    "epoches = 10\n",
    "\n",
    "median_loss = []\n",
    "mean_loss = []\n",
    "nine_loss = []\n",
    "all_loss = []\n",
    "\n",
    "for epoch in range(0, epoches):\n",
    "    print(\"=====>第{}轮训练开始：\".format(epoch))\n",
    "    los = []\n",
    "    for j in shuffle_list:\n",
    "        x_batch_train = f[j, :, :, :, :]\n",
    "        y_batch_train = l[j, :, :]\n",
    "        t_batch_time = t[j, :]\n",
    "        m_batch_time = m[j, :]\n",
    "        x_batch_train = tf.cast(x_batch_train, tf.float32)\n",
    "        y_batch_train = tf.cast(y_batch_train, tf.float32)\n",
    "        t_batch_time = tf.cast(t_batch_time, tf.float32)\n",
    "        m_batch_time = tf.cast(m_batch_time, tf.float32)\n",
    "        batch_input = []\n",
    "        #         print (x_batch_train.shape, t_batch_time.shape)\n",
    "        batch_input.append(x_batch_train)\n",
    "        batch_input.append(t_batch_time)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(raw_model.trainable_variables)\n",
    "            pred = raw_model(batch_input)\n",
    "            pred = tf.reshape(pred, [BATCH_S * Max_S, 2])\n",
    "\n",
    "            loss = loss_function_const(y_batch_train, pred, t_batch_time, m_batch_time, yscaler, m_hist)\n",
    "\n",
    "        grads = tape.gradient(loss, raw_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, raw_model.trainable_variables))\n",
    "        loss_metric(loss)\n",
    "        los.append(loss)\n",
    "        if j % 50 == 0:\n",
    "            print('step %s: loss = %s' % (j, loss))\n",
    "\n",
    "    all_loss.append(los)\n",
    "    # if epoch % 10 == 0 and epoch <= 400:\n",
    "    pred, true = predict_t(raw_model, ef, el, et, lox_sel)\n",
    "    te_err = sorted(f_error(pred, true, yscaler))\n",
    "    median_loss.append(np.median(te_err))\n",
    "    mean_loss.append(np.mean(te_err))\n",
    "    nine_loss.append(te_err[int(len(te_err) * 0.9)])\n",
    "    print(\"轮数：{}, 中位数误差:{}, 平均误差：{}, 90%误差：{}\".format(epoch, np.median(te_err), np.mean(te_err), te_err[int(len(te_err) * 0.9)]))\n",
    "\n",
    "    \n",
    "\n",
    "    # if epoch > 400:\n",
    "    #     pred, true = predict_t(raw_model, ef, el, et, lox_sel)\n",
    "    #     te_err = sorted(f_error(pred, true, yscaler))\n",
    "    #     print(epoch, np.median(te_err), np.mean(te_err), te_err[int(len(te_err) * 0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model.save(\"./model/model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import os  # 系统环境变量失效\n",
    "os.environ[\"PATH\"] += os.pathsep + r'D:/AppData/graphviz/bin'  # 解决问题\n",
    "\n",
    "\n",
    "# 绘制模型\n",
    "plot_model(raw_model,to_file='cnn_model.png', show_shapes=True, show_layer_names='True', rankdir='TB') # LR\n",
    "\n",
    "plt.figure(figsize=(80,80))\n",
    "img = plt.imread('cnn_model.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(median_loss)\n",
    "print(mean_loss)\n",
    "print(nine_loss)\n",
    "print(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_loss = []\n",
    "for row in all_loss:\n",
    "    r = []\n",
    "    for d in row:\n",
    "        r.append(d.numpy())\n",
    "    new_loss.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "filePath = './loss/all.csv'\n",
    "rows = zip(median_loss,mean_loss,nine_loss)\n",
    "with open(filePath, \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_as_csv(array,path):\n",
    "    \"\"\"将数组保存在.csv文件中\"\"\"\n",
    "    data = pd.DataFrame(array)\n",
    "    data.to_csv(path)\n",
    "\n",
    "save_as_csv(mean_loss, './loss/mean_loss.csv')\n",
    "save_as_csv(median_loss, './loss/median_loss.csv')\n",
    "save_as_csv(nine_loss, './loss/nine_loss.csv')\n",
    "save_as_csv(new_loss, './loss/all_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 读取loss数据进行可视化\n",
    "import pandas as pd\n",
    "\n",
    "mean_loss = pd.read_csv(\"./loss/mean_loss.csv\")\n",
    "mean_loss = list(mean_loss['0'])\n",
    "median_loss = pd.read_csv(\"./loss/median_loss.csv\")\n",
    "median_loss = list(median_loss['0'])\n",
    "nine_loss = pd.read_csv(\"./loss/nine_loss.csv\")\n",
    "nine_loss = list(nine_loss['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i for i in range(10)]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Attention LSTM Epoch-Loss\")\n",
    "\n",
    "plt.plot(x, mean_loss, label='mean_loss')\n",
    "plt.plot(x, median_loss, label='median_loss')\n",
    "plt.plot(x, nine_loss, label='nine_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_loss = pd.read_csv(\"./loss/all_loss.csv\")\n",
    "print(all_loss.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = [i for i in range(10)]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"realtime loss\")\n",
    "for i in range(10):\n",
    "    aloss = list(all_loss[str(i)])\n",
    "    plt.plot(x, aloss, label='step '+str(i))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 读取loss数据进行可视化\n",
    "import pandas as pd\n",
    "\n",
    "mean_loss = pd.read_csv(\"./loss/mean_loss.csv\")\n",
    "mean_loss = list(mean_loss['0'])\n",
    "median_loss = pd.read_csv(\"./loss/median_loss.csv\")\n",
    "median_loss = list(median_loss['0'])\n",
    "nine_loss = pd.read_csv(\"./loss/nine_loss.csv\")\n",
    "nine_loss = list(nine_loss['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i for i in range(10)]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Attention LSTM Epoch-Loss\")\n",
    "\n",
    "plt.plot(x, mean_loss, label='mean_loss')\n",
    "plt.plot(x, median_loss, label='median_loss')\n",
    "plt.plot(x, nine_loss, label='nine_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_loss = pd.read_csv(\"./loss/all_loss.csv\")\n",
    "print(all_loss.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = [i for i in range(100)]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"realtime loss\")\n",
    "for i in range(10):\n",
    "    aloss = list(all_loss[str(i)])\n",
    "    plt.plot(x, aloss, label='step '+str(i))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "\n",
    "mode_class_num = 3\n",
    "\n",
    "\n",
    "class Local(tf.keras.layers.Layer):\n",
    "    def __init__(self, latent_dim=64, intermediate_dim=128, name='Local', **kwargs):\n",
    "        super(Local, self).__init__(name=name, **kwargs)\n",
    "        self.cnn = tf.keras.layers.Conv2D(filters=64, kernel_size=(1, col), padding='valid',\n",
    "                                          input_shape=(7, col, 1), activation='relu', name='localcnn')\n",
    "        self.bn = tf.keras.layers.BatchNormalization(name='localbn')  # 把一个batch内的所有数据，从不规范的分布拉到正态分布\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2, name='localdrop') # 防止过拟合，随机0.2的不进行工作\n",
    "        self.reshape = tf.keras.layers.Reshape((7, 64), name='localreshape')  # 任意层之间连接\n",
    "        self.lstm = tf.keras.layers.LSTM(128, return_sequences=False, stateful=False, name='locallstm') # 输出维度？\n",
    "        self.dense_1 = tf.keras.layers.Dense(intermediate_dim, activation='relu', name='localdense1') # 输出维度128\n",
    "        self.dense_2 = tf.keras.layers.Dense(latent_dim, activation='relu', name='localdense2')       # 输出维度 64\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cnn_bn = self.bn(self.cnn(inputs))\n",
    "        cnn_drop = self.dropout(cnn_bn)\n",
    "        lstm_inp = self.reshape(cnn_drop)\n",
    "        # print (lstm_inp.shape)\n",
    "        lstm_out = self.lstm(lstm_inp)\n",
    "        lstm_drop = self.dropout(lstm_out)\n",
    "        dense_1 = self.dropout(self.dense_1(lstm_drop))\n",
    "        local_out = self.dense_2(dense_1)   # 64维度的数据\n",
    "        return local_out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Local, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_size=32, name='Attention', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.attention_size = attention_size    # 输入32维\n",
    "        self.dense_1 = tf.keras.layers.Dense(attention_size, activation='tanh', name='attndnese1')  # 全连接 32\n",
    "        self.dense_2 = tf.keras.layers.Dense(1, use_bias=False, name='attndense2')   # 全连接 1维\n",
    "\n",
    "    def call(self, inputs):\n",
    "        v = self.dense_1(inputs)\n",
    "        vu = self.dense_2(v)\n",
    "        alphas = tf.nn.softmax(vu)   # 一维数据经过 softmax  映射到0和1之间\n",
    "        output = alphas\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Attention, self).get_config()\n",
    "        config.update({\"attention_size\": self.attention_size})\n",
    "        return config\n",
    "\n",
    "\n",
    "import AttentionCell\n",
    "from imp import reload\n",
    "\n",
    "reload(AttentionCell)\n",
    "\n",
    "class Global(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_subs=Max_S, latent_dim=64, name='Global', **kwargs):\n",
    "        super(Global, self).__init__(name=name, **kwargs)\n",
    "        self.max_subs = max_subs\n",
    "\n",
    "        self.cell = AttentionCell.ALSTM_Cell(64, name='attncell')  # 注意力机制：输入64维度\n",
    "        self.rnn = tf.keras.layers.RNN(self.cell, return_sequences=True, name='attnRNN')  # RNN中使用Attention Cell\n",
    "        self.dense_1 = tf.keras.layers.Dense(64, activation='relu', \n",
    "                                             name='globaldense1')  # tf.keras.layers.BatchNormalization(), 全连接 64层\n",
    "        self.dense_2 = tf.keras.layers.Dense(32, activation='relu', name='globaldense2')   # 全连接 32 层\n",
    "        self.out = tf.keras.layers.Dense(2, activation='sigmoid', name='globalout')        # 全连接 2 层\n",
    "        self.attn = Attention(name='globalattn')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        # mask = self.mask(x)\n",
    "        h = self.rnn(x)   # 接受64维度数据\n",
    "        h_d_1 = self.dense_1(h)      # 64\n",
    "        h_d_2 = self.dense_2(h_d_1)  # 32\n",
    "        h_a = self.attn(h_d_2)       # 使用Attention  返回结果为一维数据，映射在0-1之间\n",
    "        y = self.out(h_d_2 * h_a)    # 输出2维数据     32维数据与0-1之间一个数据相乘，然后映射为2维数据\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Global, self).get_config()\n",
    "        config.update({\"max_subs\": self.max_subs})\n",
    "        return config\n",
    "\n",
    "class CNN_Lstm_model(tf.keras.Model):\n",
    "    def __init__(self, name='CNN_LISM_Model', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cnn = tf.keras.layers.Conv2D(filters=64, kernel_size=(1,5), padding='valid',\n",
    "                                          input_shape=(7, 5, 1), activation='relu')   # 输入层\n",
    "        self.bn = tf.keras.layers.BatchNormalization()  # 把一个batch内的所有数据，从不规范的分布拉到正态分布\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2) # 防止过拟合，随机0.2的不进行工作\n",
    "        self.reshape = tf.keras.layers.Reshape((7, 64))  # 任意层之间连接\n",
    "        self.lstm = tf.keras.layers.LSTM(128, return_sequences=False, stateful=False) # 输出维度\n",
    "        self.dense_1 = tf.keras.layers.Dense(128, activation='relu') # 输出维度128\n",
    "        self.dense_2 = tf.keras.layers.Dense(64, activation='relu')       # 输出维度 64\n",
    "        \n",
    "        self.dense_3 = tf.keras.layers.Dense(32, activation='tanh')   # 全连接 32 维  Attention\n",
    "        self.out = tf.keras.layers.Dense(2, activation='sigmoid')        # 全连接 2 维\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_inputs = inputs[0]\n",
    "        batch_time = inputs[1]\n",
    "        cnn_bn = self.bn(self.cnn(batch_inputs))\n",
    "        cnn_drop = self.dropout(cnn_bn)\n",
    "        lstm_inp = self.reshape(cnn_drop)\n",
    "        lstm_out = self.lstm(lstm_inp)\n",
    "        lstm_drop = self.dropout(lstm_out)\n",
    "        dense_1 = self.dropout(self.dense_1(lstm_drop))\n",
    "        x = self.dense_2(dense_1)\n",
    "        x = tf.reshape(x, [BATCH_S, Max_S, 64])\n",
    "        batch_time = tf.reshape(batch_time, [BATCH_S, Max_S, 1])\n",
    "        x = tf.keras.layers.concatenate([x, batch_time], axis=-1)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Seq, self).get_config()\n",
    "        config.update({\"local_out_dim\": self.local_out_dim})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
